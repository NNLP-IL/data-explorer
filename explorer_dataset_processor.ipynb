{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WebiksInc/data-explorer/blob/main/function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xuOxcOBD_uPn",
      "metadata": {
        "id": "xuOxcOBD_uPn"
      },
      "source": [
        "# Import libraries, and utility function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "HoDUx18hOFnR",
      "metadata": {
        "id": "HoDUx18hOFnR"
      },
      "outputs": [],
      "source": [
        "# add missing package\n",
        "try:\n",
        "    from gensim.models.nmf import Nmf\n",
        "except ImportError as e:\n",
        "    !pip install -U gensim\n",
        "\n",
        "try:\n",
        "    import langdetect\n",
        "except ImportError as e:\n",
        "    !pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f385394a",
      "metadata": {
        "id": "f385394a"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from scipy.special import rel_entr\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.nmf import Nmf\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from langdetect import detect_langs\n",
        "from operator import itemgetter\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_rows\", 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "PQqD9aql__88",
      "metadata": {
        "id": "PQqD9aql__88"
      },
      "outputs": [],
      "source": [
        "# Utility function\n",
        "\n",
        "\n",
        "def load_parquet_plot_distance(dirc: str, file: str):\n",
        "    path = os.path.join(path_save, \"without stopwords\", dirc, file)\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "\n",
        "def remove_niqqud_from_string(my_string):\n",
        "    return \"\".join([\"\" if 1456 <= ord(c) <= 1479 else c for c in my_string])\n",
        "\n",
        "\n",
        "def save_pickle(obj, file_name, dirc):\n",
        "    global path_save\n",
        "    dirc = str(dirc).split(\".\")[0]\n",
        "    with open(os.path.join(path_save, dirc, file_name), \"wb\") as handle:\n",
        "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def save_parquet(obj, file_name, dirc):\n",
        "    global path_save\n",
        "    dirc = str(dirc).split(\".\")[0]\n",
        "    obj.to_parquet(os.path.join(path_save, dirc, file_name))\n",
        "\n",
        "\n",
        "def update_word_dict(line, n, words):\n",
        "    # Update the word dict for count ngram on the corpus.\n",
        "    line = line.split(\" \")\n",
        "    line = [i for i in line if (i != '\"') & (len(i) > 0)]\n",
        "    line = list(map(remove_niqqud_from_string, line))\n",
        "    n_grams = zip(*(line[i:] for i in range(n)))\n",
        "    temp = Counter(n_grams)\n",
        "    words.update(temp)\n",
        "\n",
        "\n",
        "def detect_langs_fun(x):\n",
        "    # The warper for detect_langs function will not stop the function if it returns an error.\n",
        "    try:\n",
        "        return detect_langs(x)[0].lang\n",
        "    except:\n",
        "        return \"error\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a14ZLwBI_3Xn",
      "metadata": {
        "id": "a14ZLwBI_3Xn"
      },
      "source": [
        "# main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RkuTSwDZUNvS",
      "metadata": {
        "id": "RkuTSwDZUNvS"
      },
      "outputs": [],
      "source": [
        "# Change to the path to save the data\n",
        "path_save = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "re_nVjMkeECW",
      "metadata": {
        "id": "re_nVjMkeECW"
      },
      "outputs": [],
      "source": [
        "# Dummy database. We sampled 10 entries from Wikipedia, and performed the following manipulations: remove stopwords, lemmatization (with trankit) and stopwords + lemmatization\n",
        "\n",
        "# flag = 'raw'\n",
        "# flag = 'lemmatization'\n",
        "flag = 'without stopwords'\n",
        "# flag = 'without stopwords & lemmatization'\n",
        "\n",
        "if flag == 'raw':\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/NNLP-IL/data-explorer/main/wiki%20samples/wiki%20row.csv',sep = '\\t')\n",
        "elif flag == 'lemmatization':\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/NNLP-IL/data-explorer/main/wiki%20samples/wiki%20lemmatization.csv',sep = '\\t')\n",
        "elif flag == 'without stopwords':\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/NNLP-IL/data-explorer/main/wiki%20samples/wiki%20without%20stopwords.csv',sep = '\\t')\n",
        "elif flag == 'without stopwords & lemmatization':\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/NNLP-IL/data-explorer/main/wiki%20samples/wiki%20lemmatization%20and%20without%20stopwords.csv',sep = '\\t')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8ba7fd93",
      "metadata": {
        "id": "8ba7fd93",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# function distance plot\n",
        "\n",
        "\n",
        "def lng_distubtion_function(dis_word1, dis_word2) -> float:\n",
        "    n = 5000  # tunable parameter. the number of words to keep\n",
        "\n",
        "    df_a = dis_word1[\n",
        "        (dis_word1[\"word\"].isin(dis_word1[\"word\"].head(n)))\n",
        "        | (dis_word1[\"word\"].isin(dis_word2[\"word\"].head(n)))\n",
        "    ]\n",
        "    df_b = dis_word2[\n",
        "        (dis_word2[\"word\"].isin(dis_word2[\"word\"].head(n)))\n",
        "        | (dis_word2[\"word\"].isin(dis_word1[\"word\"].head(n)))\n",
        "    ]\n",
        "    df_a = df_a[df_a[\"word\"].isin(df_b[\"word\"])]\n",
        "    df_b = df_b[df_b[\"word\"].isin(df_a[\"word\"])]\n",
        "\n",
        "    df_a = df_a.sort_values(\"word\")[\"frequency\"]\n",
        "    df_b = df_b.sort_values(\"word\")[\"frequency\"]\n",
        "    return sum(rel_entr(list(df_a), list(df_b))) + sum(rel_entr(list(df_b), list(df_a)))\n",
        "\n",
        "\n",
        "def distance_plot(main_path):\n",
        "    # Calculates the distance between each pair of corpuses\n",
        "\n",
        "    corpus = np.sort(os.listdir(os.path.join(path_save, \"without stopwords\")))\n",
        "    corpus = [x.split(\".\")[0] for x in corpus]  # remove csv ending\n",
        "\n",
        "    result = pd.DataFrame(index=corpus, columns=corpus)\n",
        "    for index in itertools.combinations(corpus, 2):\n",
        "        dis_word1 = load_parquet_plot_distance(index[0], \"ngram 1\")\n",
        "        dis_word1[\"frequency\"] = dis_word1[\"frequency\"] / dis_word1[\"frequency\"].sum()\n",
        "        dis_word2 = load_parquet_plot_distance(index[1], \"ngram 1\")\n",
        "        dis_word2[\"frequency\"] = dis_word2[\"frequency\"] / dis_word2[\"frequency\"].sum()\n",
        "        result.loc[index[0], index[1]] = lng_distubtion_function(dis_word1, dis_word2)\n",
        "    with open(\n",
        "        os.path.join(base_path, \"corpus\", \"main\", \"distance_plot.pickle\"), \"wb\"\n",
        "    ) as handle:\n",
        "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dd748978",
      "metadata": {
        "id": "dd748978"
      },
      "outputs": [],
      "source": [
        "# Function process data\n",
        "\n",
        "\n",
        "def number_lines(df: pd.Series) -> int:\n",
        "    num_line = df.shape[0]\n",
        "    return num_line\n",
        "\n",
        "\n",
        "def len_word_in_line(line: str) -> list:\n",
        "    # The length of each word in the line\n",
        "    line = line.replace(\".\", \" \")\n",
        "    line = \" \".join(line.split())  # remove multiple spaces\n",
        "    line = line.split(\" \")\n",
        "    return [len(i) for i in line]\n",
        "\n",
        "\n",
        "def avg_len_word(df: pd.Series) -> int:\n",
        "    # Return the average length of all words in the series\n",
        "    lines = df.apply(lambda x: len_word_in_line(x))\n",
        "    avg_word = np.mean([x for xs in lines for x in xs])\n",
        "    return avg_word\n",
        "\n",
        "\n",
        "def number_word_in_line(df: pd.Series) -> dict:\n",
        "    df = df.apply(lambda x: \" \".join(x.split()))  # remove multiple spaces\n",
        "    result = df.apply(lambda x: len(x.split(\" \")))\n",
        "    return {\"mean\": np.round(result.mean(), 2), \"median\": result.median()}\n",
        "\n",
        "\n",
        "def char_freq(df: pd.Series) -> dict:\n",
        "    # Distribution of all characters in the corpus\n",
        "    x = df.tolist()\n",
        "    x = Counter(\"\".join(x))\n",
        "    x = dict(sorted(x.items(), key=lambda item: item[1]))\n",
        "    return x\n",
        "\n",
        "\n",
        "def lexical_density(dirc: str):\n",
        "    # check for lexical_density. we defined list of stop words here:\n",
        "    # https://github.com/NNLP-IL/Stop-Words-Hebrew\n",
        "    # with the following parts of speech: DET,ADP,PRON, CCONJ, SCONJ\n",
        "    dct_lxl = pd.read_csv(\n",
        "        'https://raw.githubusercontent.com/NNLP-IL/data-explorer/main/Lexical_density.txt'\n",
        "    )\n",
        "    dct_lxl = dct_lxl[\"stopswords\"].to_list()\n",
        "    a = os.path.join(path_save, dirc, \"ngram 1.pickle\")\n",
        "    path_parquet = os.path.join(path_save, dirc, \"ngram 1\")\n",
        "    word_freq = pd.read_parquet(path_parquet)\n",
        "    lxl = (\n",
        "        word_freq[word_freq[\"word\"].isin(dct_lxl)][\"frequency\"].sum()\n",
        "        / word_freq[\"frequency\"].sum()\n",
        "    )\n",
        "    return lxl\n",
        "\n",
        "\n",
        "def identity_duplicate_line(df: pd.Series) -> int:\n",
        "    return df.duplicated().sum()\n",
        "\n",
        "\n",
        "def detect_lang_croup(df: pd.Series) -> None:\n",
        "    # Verify whether the text contains any other languages besides Hebrew. We conduct a word-level check since some languages share characters.\n",
        "    # In Explorer, we aggregate all languages other than Hebrew, so we are not able to perform word-level testing for those languages.\n",
        "    # There is an easier solution to this problem by using regax to check the character level\n",
        "    result = df.apply(lambda x: detect_langs_fun(x))\n",
        "    result = result.value_counts()\n",
        "    result = result.to_frame().reset_index()\n",
        "    return result\n",
        "\n",
        "\n",
        "def Zipf_law(df: pd.Series) -> pd.DataFrame:\n",
        "    # Calculation zipf law.\n",
        "\n",
        "    # Preparations\n",
        "    df = df.replace(\"[^\\u0590-\\u05fe]\", \" \", regex=True)\n",
        "\n",
        "    # find freq words\n",
        "    vec = CountVectorizer(ngram_range=(1, 1)).fit(df)\n",
        "    bag_of_words = vec.transform(df)\n",
        "    dat = pd.DataFrame(\n",
        "        zip(vec.get_feature_names(), bag_of_words.sum(axis=0).tolist()[0])\n",
        "    )\n",
        "    dat.columns = [\"words\", \"freq\"]\n",
        "    dat = dat.sort_values(\"freq\", ascending=False)\n",
        "    dat = dat.reset_index(drop=True).reset_index(drop=False)\n",
        "    dat = dat.rename(columns={\"index\": \"rank\"})\n",
        "    dat[\"rank\"] = dat[\"rank\"] + 1\n",
        "\n",
        "    # calculate zipf law\n",
        "    dat[\"freq\"] = dat[\"freq\"] / dat[\"freq\"].sum()\n",
        "    dat[\"Zipf\"] = dat.loc[0, \"freq\"] / dat[\"rank\"]\n",
        "    dat[\"Zipf\"] = dat[\"Zipf\"] / dat[\"Zipf\"].sum()\n",
        "\n",
        "    return dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "hDYR_FfzYUS-",
      "metadata": {
        "id": "hDYR_FfzYUS-"
      },
      "outputs": [],
      "source": [
        "# Extract topics\n",
        "def coherence_check(corpus, index, dictionary, texts) -> int:\n",
        "    nmf = Nmf(\n",
        "        corpus=corpus,\n",
        "        num_topics=index,\n",
        "        id2word=dictionary,\n",
        "        chunksize=1000,\n",
        "        passes=5,\n",
        "        kappa=0.1,\n",
        "        minimum_probability=0.01,\n",
        "        w_max_iter=300,\n",
        "        w_stop_condition=0.0001,\n",
        "        h_max_iter=100,\n",
        "        h_stop_condition=0.001,\n",
        "        eval_every=10,\n",
        "        normalize=True,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    cm = CoherenceModel(model=nmf, texts=texts, dictionary=dictionary, coherence=\"c_v\")\n",
        "    return round(cm.get_coherence(), 5)\n",
        "\n",
        "\n",
        "def pick_best_number_topic(df: pd.DataFrame) -> int:\n",
        "    # Decide on the best topic number.\n",
        "    # In this case, we mean that it is the highest, but not higher than a small number topic by 1.1.\n",
        "    df[\"shift\"] = df[\"num\"].shift(1)\n",
        "    df[\"shift\"] = df[\"shift\"] > df[\"num\"]\n",
        "    df[\"shift\"] = df[\"shift\"].cumsum()\n",
        "    df = df[df.index == df[\"shift\"]]\n",
        "\n",
        "    df[\"shift\"] = df[\"score\"].shift(1)\n",
        "    df[\"shift\"] = df[\"shift\"] / df[\"score\"]\n",
        "    df[\"improve 10%\"] = df[\"shift\"] > 1.1\n",
        "    df[\"temp\"] = df[\"improve 10%\"].cumsum()\n",
        "    best_num_topics = df[df[\"temp\"] == df[\"temp\"].min()].tail(1)[\"num\"].item()\n",
        "    return best_num_topics\n",
        "\n",
        "\n",
        "def fine_tuning_number_topics(corpus, dictionary, texts) -> int:\n",
        "    topic_nums = list(np.arange(5, 45 + 1, 5))\n",
        "    coherence_scores = []\n",
        "    for index in topic_nums:\n",
        "        print('check '  + str(index) + ' topics')\n",
        "        temp = coherence_check(corpus, index, dictionary, texts)\n",
        "        coherence_scores.append(temp)\n",
        "\n",
        "    scores = list(zip(topic_nums, coherence_scores))\n",
        "    temp = pd.DataFrame(\n",
        "        sorted(scores, key=itemgetter(1), reverse=True), columns=[\"num\", \"score\"]\n",
        "    )\n",
        "    k = pick_best_number_topic(temp)\n",
        "    return k\n",
        "\n",
        "\n",
        "def topic_model(df: pd.Series):\n",
        "    # prefom NMF  topic analysis\n",
        "    texts = df.apply(lambda x: x.split(\" \"))\n",
        "\n",
        "    vector = TfidfVectorizer(\n",
        "        min_df=3,\n",
        "        max_df=0.85,\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        preprocessor=\" \".join,\n",
        "    )\n",
        "    tfidf = vector.fit_transform(texts)\n",
        "    dictionary = Dictionary(texts)\n",
        "    dictionary.filter_extremes(no_below=3, no_above=0.85, keep_n=5000)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    k_topic = fine_tuning_number_topics(corpus, dictionary, texts)\n",
        "\n",
        "    terms = vector.get_feature_names()\n",
        "    result = []\n",
        "\n",
        "    nmf = NMF(n_components=k_topic)\n",
        "    nmf.fit(tfidf)\n",
        "    for i in range(0, k_topic):\n",
        "        word_list = []\n",
        "        for j in nmf.components_.argsort()[i, -9:-1]:  # Specifies the number of words.\n",
        "            word_list.append(terms[j])\n",
        "        result.append(word_list)\n",
        "    return pd.DataFrame(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bPD8nvbeYLKc",
      "metadata": {
        "id": "bPD8nvbeYLKc"
      },
      "outputs": [],
      "source": [
        "# Ngram function\n",
        "def calculation_gini_index(dat: pd.Series) -> int:\n",
        "    sorted_x = np.sort(dat)\n",
        "    n = len(sorted_x)\n",
        "    cumx = np.cumsum(sorted_x, dtype=float)\n",
        "    gini = (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
        "    return gini\n",
        "\n",
        "\n",
        "def ngram_fun(df: pd.Series, ngram: int, name: str) -> dict:\n",
        "    # Custom function to ngram. We created a function to address the memory issue that arises in large databases.\n",
        "    # By using our function, you are able to go line by line without having to worry about memory issues\n",
        "    # We also delete all words that appear one time. Most of it is junk, and it takes a great deal of memory.\n",
        "\n",
        "    # Preparations\n",
        "    df = df.replace('[^\\u0590-\\u05fe\"]', \" \", regex=True)  # keep only hebrew character\n",
        "    # df = df.str.replace('[^\\w\\s\"]',' ') # remove punctuation\n",
        "\n",
        "    # main code\n",
        "    words = Counter()  # create dictionary\n",
        "    not_important = df.astype(str).progress_apply(\n",
        "        lambda x: update_word_dict(x, ngram, words)\n",
        "    )  # update the dictionary\n",
        "    full_size = len(words)\n",
        "    # return words\n",
        "    ngram_dict = {\n",
        "        \" \".join(k): v for k, v in words.items() if v != 1\n",
        "    }  # keep only appear twice or more\n",
        "    word_one_apper_shape = full_size - len(ngram_dict)\n",
        "\n",
        "    ngram_df = pd.DataFrame(ngram_dict.items(), columns=[\"word\", \"frequency\"])\n",
        "    ngram_df = ngram_df.sort_values(\"frequency\", ascending=False)\n",
        "\n",
        "    # gini index\n",
        "    gini = calculation_gini_index(ngram_df[\"frequency\"])\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"shape unique words\": word_one_apper_shape + ngram_df.shape[0],\n",
        "        \"shape appeared once\": word_one_apper_shape,\n",
        "        \"Percent appeared once\": word_one_apper_shape\n",
        "        / (word_one_apper_shape + ngram_df.shape[0]),\n",
        "        \"top unqiue words\": ngram_df,\n",
        "        \"type-token ratio\": (word_one_apper_shape + ngram_df.shape[0])\n",
        "        / (word_one_apper_shape + ngram_df[\"frequency\"].sum()),\n",
        "        \"Number Of Words\": (word_one_apper_shape + ngram_df[\"frequency\"].sum()),\n",
        "        \"gini\": gini,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b4fb96",
      "metadata": {
        "id": "49b4fb96",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Runs all the above functions, and saves the results\n",
        "\n",
        "\n",
        "def loop_on_function(df: pd.DataFrame, dirc: str):\n",
        "    df = df[df[\"line\"].notnull()]\n",
        "    df[\"line\"] = df[\"line\"].astype(\"str\")\n",
        "\n",
        "    # create folder\n",
        "    global path_save\n",
        "    if not os.path.exists(os.path.join(path_save, dirc)):\n",
        "      os.mkdir(os.path.join(path_save, dirc))\n",
        "    # len_word\n",
        "    print(\"avg_len_word\")\n",
        "    temp = avg_len_word(df[\"line\"])\n",
        "    save_pickle(temp, \"len words.pickle\", dirc)\n",
        "    # Zipf_law\n",
        "    print(\"Zipf_law\")\n",
        "    temp = Zipf_law(df[\"line\"])\n",
        "    save_pickle(temp, \"Zipf law.pickle\", dirc)\n",
        "    # number lines\n",
        "    print(\"number lines\")\n",
        "    temp = number_lines(df[\"line\"])\n",
        "    save_pickle(temp, \"number lines.pickle\", dirc)\n",
        "    # Character distribution\n",
        "    print(\"Character distribution\")\n",
        "    temp = char_freq(df[\"line\"])\n",
        "    save_pickle(temp, \"Character distribution.pickle\", dirc)\n",
        "    # ngram\n",
        "    print(\"ngram\")\n",
        "    temp = ngram_fun(df[\"line\"], 1, dirc.split(\".\")[0])\n",
        "    top_words = temp.pop(\"top unqiue words\")\n",
        "    save_parquet(top_words, \"ngram 1\", dirc)\n",
        "    save_pickle(temp, \"stat.pickle\", dirc)\n",
        "    for x in range(2, 6):\n",
        "        temp = ngram_fun(df[\"line\"], x, dirc.split(\".\")[0])\n",
        "        top_words = temp.pop(\"top unqiue words\")\n",
        "        save_parquet(top_words, \"ngram \" + str(x), dirc)\n",
        "    # identity duplicate line\n",
        "    print(\"identity duplicate\")\n",
        "    temp = identity_duplicate_line(df[\"line\"])\n",
        "    save_pickle(temp, \"identity duplicate line.pickle\", dirc)\n",
        "    # topic model\n",
        "    print(\"topic model\")\n",
        "    temp = topic_model(df[\"line\"])\n",
        "    save_pickle(temp, \"topic model.pickle\", dirc)\n",
        "    # number word in line\n",
        "    print(\"number word in line\")\n",
        "    temp = number_word_in_line(df[\"line\"])\n",
        "    save_pickle(temp, \"number word in line.pickle\", dirc)\n",
        "    # Language recognition in corpus\n",
        "    print(\"Language recognition in corpus\")\n",
        "    temp = detect_lang_croup(df[\"line\"])\n",
        "    save_pickle(temp, \"Language recognition in corpus.pickle\", dirc)\n",
        "    # lexical_density\n",
        "    print(\"lexical_density\")\n",
        "    temp = lexical_density(dirc.split(\".\")[0])\n",
        "    save_pickle(temp, \"lexical_density.pickle\", dirc)\n",
        "\n",
        "\n",
        "loop_on_function(df,flag)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "function.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('test')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb0112c456f82d9d2ffc2ba40e932c79270a28c5f552c1ea081145b825aa262c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
