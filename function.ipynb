{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WebiksInc/data-explorer/blob/main/function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f385394a",
      "metadata": {
        "id": "f385394a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import pickle\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import IPython.display\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.special import rel_entr\n",
        "from gensim.corpora import Dictionary\n",
        "from sklearn.decomposition import NMF\n",
        "from gensim.models.nmf import Nmf\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "import re\n",
        "from langdetect import detect_langs\n",
        "from operator import itemgetter\n",
        "import itertools\n",
        "\n",
        "# base_path = r'/home/ubuntu/voila/notebooks'\n",
        "base_path = r'C:\\Users\\user\\My Drive (roei_shlezinger@webiks.com)'\n",
        "# base_path = r'/content/drive/MyDrive'\n",
        "# path_corp = base_path + r'\\work\\with stopwords'\n",
        "# path_main = base_path + r'\\work\\for display\\with stopwords'\n",
        "# path_save = base_path + r'\\corpus\\with stopwords'\n",
        "\n",
        "path_corp = base_path + r'\\work\\without stopwords'\n",
        "path_main = base_path + r'\\work\\without stopwords'\n",
        "path_save = base_path + r'\\corpus\\without stopwords'\n",
        "\n",
        "\n",
        "# path_corp = base_path + r'\\work\\without stopwords and stanza'\n",
        "# path_main = base_path + r'\\work\\for display\\with stopwords'\n",
        "# path_save = base_path + r'\\corpus\\without stopwords and stanza'\n",
        "\n",
        "one_apper = 0\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba7fd93",
      "metadata": {
        "scrolled": true,
        "id": "8ba7fd93"
      },
      "outputs": [],
      "source": [
        "# function distance plot \n",
        "\n",
        "def load_pickle_plot_distance(dirc:str,file: str):\n",
        "    path = os.path.join(base_path,'corpus','without stopwords',dirc,file)\n",
        "    with open(path, 'rb') as handle:\n",
        "        return pickle.load(handle)\n",
        "\n",
        "\n",
        "def lng_distubtion_function(dis_word1,dis_word2):\n",
        "    n = 5000 # tunable parameter. the number of words to keep\n",
        "    \n",
        "    df_a = dis_word1[(dis_word1['word'].isin(dis_word1['word'].head(n))) | (dis_word1['word'].isin(dis_word2['word'].head(n)))]\n",
        "    df_b = dis_word2[(dis_word2['word'].isin(dis_word2['word'].head(n))) | (dis_word2['word'].isin(dis_word1['word'].head(n)))]\n",
        "    df_a = df_a[df_a['word'].isindf_bb['word'])]\n",
        "    df_b = df_b[df_b['word'].isin(df_a['word'])]\n",
        "\n",
        "    \n",
        "    df_a = df_a.sort_values('word')['frequency']\n",
        "    df_b = df_b.sort_values('word')['frequency']\n",
        "    return sum(rel_entr(list(df_a),list(df_b))) + sum(rel_entr(list(df_b),list(df_a)))\n",
        "\n",
        "def distance_plot(main_path):\n",
        "    # Calculates the distance between each pair of corpuses\n",
        "    corpus = np.sort(os.listdir(main_path + '/corpus/without stopwords'))\n",
        "    corpus = [x.split('.')[0] for x in corpus] #remove csv ending\n",
        "     \n",
        "    result = pd.DataFrame(index = corpus,columns = corpus)\n",
        "    for index in itertools.combinations(corpus, 2):\n",
        "        dis_word1 = load_pickle_plot_distance(index[0],'ngram 1.pickle')['top unqiue words']\n",
        "        dis_word1['frequency'] = dis_word1['frequency']/dis_word1['frequency'].sum()\n",
        "        dis_word2 = load_pickle_plot_distance(index[1],'ngram 1.pickle')['top unqiue words']\n",
        "        dis_word2['frequency']  = dis_word2['frequency']/dis_word2['frequency'].sum()\n",
        "        result.loc[index[0],index[1]] = lng_distubtion_function(dis_word1,dis_word2)\n",
        "\n",
        "    with open(path_save + '/distance_plot.pickle', 'wb') as handle:\n",
        "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# distance_plot(base_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd748978",
      "metadata": {
        "id": "dd748978",
        "outputId": "68a3b69f-a1e3-4420-c772-eec16b864ca3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5304/231804044.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# function process data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mnumber_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mnum_line\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnum_line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# function process data\n",
        "\n",
        "def number_lines(df:pd.Series) -> int:\n",
        "    num_line = df.shape[0]\n",
        "    return num_line\n",
        "\n",
        "def len_word_in_line(line: str) -> list:\n",
        "    \"\"\"\n",
        "    The length of each word in the line\n",
        "    \"\"\"\n",
        "    line = line.replace(\".\", \" \")\n",
        "    line = ' '.join(line.split()) # remove multiple spaces\n",
        "    line = line.split(' ')\n",
        "    return [len(i) for i in line]\n",
        "\n",
        "def avg_len_word(df:pd.Series) -> int:\n",
        "    \"\"\"\n",
        "    Return the average length of all words in the series\n",
        "    \"\"\"\n",
        "    lines = df.apply(lambda x: len_word_in_line(x))\n",
        "    avg_word = np.mean([x for xs in lines for x in xs])\n",
        "    return avg_word\n",
        "\n",
        "\n",
        "\n",
        "def number_word_in_line(df:pd.Series) -> dict:\n",
        "    df = df.apply(lambda x: ' '.join(x.split())) # remove multiple spaces\n",
        "    result = df.apply(lambda x: len(x.split(' ')))\n",
        "    return {'mean' : np.round(result.mean(),2),\n",
        "            'median' : result.median()}\n",
        "\n",
        "\n",
        "def char_freq(df:pd.Series) -> dict:\n",
        "    \"\"\" \n",
        "    Distribution of all characters in the corpus\n",
        "    \"\"\"\n",
        "    x = df.tolist()\n",
        "    x = Counter(''.join(x))\n",
        "    x = dict(sorted(x.items(), key=lambda item: item[1]))\n",
        "    return x\n",
        "\n",
        "\n",
        "def keep_occur_twice(value,key,ngram) -> bool:\n",
        "    \"\"\"\n",
        "    For reasons of efficiency, we will not save words that appear once. \n",
        "    A helper function that returns true whenever the value appears more than once\n",
        "    \"\"\"\n",
        "    if (value > 1) and (len(key) == ngram) and all([len(key[i])>0 for i in range(0,ngram)]):\n",
        "        return True\n",
        "    elif (value == 1) and (len(key) == ngram) and all([len(key[i])>0 for i in range(0,ngram)]):\n",
        "        global one_apper\n",
        "        one_apper+=1\n",
        "        return False\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def remove_niqqud_from_string(my_string):\n",
        "    return \"\".join([\"\" if 1456 <= ord(c) <= 1479 else c for c in my_string])\n",
        "\n",
        "def update_word_dict(line,n,words):\n",
        "    \"\"\"\n",
        "    Update the word dict for count ngram on the corpus.\n",
        "    \"\"\"\n",
        "    line = line.split(' ')\n",
        "    line = list(map(remove_niqqud_from_string,line))\n",
        "    n_grams = zip(*(line[i:] for i in range(n)))\n",
        "    temp = Counter(n_grams)\n",
        "    words.update(temp)\n",
        "\n",
        "\n",
        "def calculation_gini_index(dat : pd.Series) -> int:\n",
        "    sorted_x = np.sort(dat)\n",
        "    n = len(sorted_x)\n",
        "    cumx = np.cumsum(sorted_x, dtype=float)\n",
        "    gini = ((n + 1 - 2 * np.sum(cumx) / cumx[-1])/ n)\n",
        "    return gini\n",
        "\n",
        "def ngram_fun(df:pd.Series, ngram:int,name:str) -> dict:\n",
        "    \"\"\"\n",
        "    Custom function to ngram. We created a function to address the memory issue that arises in large databases.\n",
        "    By using our function, you are able to go line by line without having to worry about memory issues\n",
        "    We also delete all words that appear one time. Most of it is junk, and it takes a great deal of memory.\n",
        "    \"\"\"\n",
        "\n",
        "    # Preparations\n",
        "    global one_apper\n",
        "    one_apper = 0\n",
        "    df = df.replace('[^\\u0590-\\u05fe\"]',' ',regex=True) # keep only hebrew character\n",
        "    df = df.str.replace('[^\\w\\s]',' ') # remove punctuation\n",
        "\n",
        "    # main code\n",
        "    words = Counter() #create dictionary\n",
        "    not_important = df.astype(str).progress_apply(lambda x: update_word_dict(x,ngram,words)) # update the dictionary\n",
        "    ngram_dict = [(' '.join(key),value) for key, value in words.items() if keep_occur_twice(value,key,ngram)] # keep only appear twice or more\n",
        "    ngram_df = pd.DataFrame(ngram_dict)\n",
        "    ngram_df.columns=['word','frequency']\n",
        "    ngram_df = ngram_df.sort_values('frequency',ascending = False)\n",
        "\n",
        "    # gini index\n",
        "    gini = calculation_gini_index(ngram_df['frequency'])\n",
        "\n",
        "    return({\n",
        "        'name' : name,\n",
        "        'shape unique words' : one_apper + ngram_df.shape[0],\n",
        "        'shape appeared once' : one_apper,\n",
        "        'Percent appeared once' : one_apper/(one_apper + ngram_df.shape[0]),\n",
        "        'top unqiue words' : ngram_df,\n",
        "        'type-token ratio' : (one_apper + ngram_df.shape[0])/(one_apper + ngram_df['frequency'].sum()),\n",
        "        'Number Of Words' : (one_apper + ngram_df['frequency'].sum()),\n",
        "        'gini' : gini\n",
        "                }\n",
        "          )\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "def identity_duplicate_line(df:pd.Series) -> int:\n",
        "    return df.duplicated().sum()\n",
        "\n",
        "def coherence_check(corpus,index,dictionary,texts) -> int:\n",
        "    nmf = Nmf(\n",
        "                corpus=corpus,\n",
        "                num_topics=index,\n",
        "                id2word=dictionary,\n",
        "                chunksize=1000,\n",
        "                passes=5,\n",
        "                kappa=.1,\n",
        "                minimum_probability=0.01,\n",
        "                w_max_iter=300,\n",
        "                w_stop_condition=0.0001,\n",
        "                h_max_iter=100,\n",
        "                h_stop_condition=0.001,\n",
        "                eval_every=10,\n",
        "                normalize=True,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "    cm = CoherenceModel(\n",
        "                model=nmf,\n",
        "                texts=texts,\n",
        "                dictionary=dictionary,\n",
        "                coherence='c_v'\n",
        "            )\n",
        "    return round(cm.get_coherence(), 5)\n",
        "\n",
        "    \n",
        "\n",
        "def pick_best_number_topic(df:pd.DataFrame) -> int:\n",
        "    \"\"\"\n",
        "    Decide on the best topic number.\n",
        "    In this case, we mean that it is the highest, but not higher than a small number topic by 1.1.\n",
        "    \"\"\"\n",
        "    df['shift'] = df['num'].shift(1)\n",
        "    df['shift'] = df['shift'] > df['num']\n",
        "    df['shift'] = df['shift'].cumsum()\n",
        "    df = df[df.index == df['shift']]\n",
        "\n",
        "    df['shift'] = df['score'].shift(1)\n",
        "    df['shift'] = df['shift']/df['score']\n",
        "    df['improve 10%'] = df['shift'] > 1.1\n",
        "    df['temp'] = df['improve 10%'].cumsum()\n",
        "    best_num_topics = df[df['temp'] == df['temp'].min()].tail(1)['num'].item()\n",
        "    return best_num_topics\n",
        "\n",
        "\n",
        "def fine_tuning_number_topics(corpus,dictionary,texts) -> int:\n",
        "    topic_nums = list(np.arange(5, 45 + 1, 5))\n",
        "    coherence_scores = []\n",
        "    for index in topic_nums:\n",
        "        print(index)\n",
        "        temp = coherence_check(corpus,index,dictionary,texts)\n",
        "        coherence_scores.append(temp)\n",
        "\n",
        "    scores = list(zip(topic_nums, coherence_scores))\n",
        "    temp = pd.DataFrame(sorted(scores, key=itemgetter(1), reverse=True),columns= ['num','score'])\n",
        "    k = pick_best_number_topic(temp)\n",
        "    print(k)\n",
        "    return k\n",
        "\n",
        "\n",
        "\n",
        "def topic_model(df:pd.Series):\n",
        "    \"\"\"\n",
        "    prefom NMF  topic analysis\n",
        "    \"\"\"\n",
        "    texts = df.apply(lambda x : x.split(' '))\n",
        "    \n",
        "    vector = TfidfVectorizer(\n",
        "        min_df=3,\n",
        "        max_df=0.85,\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        preprocessor=' '.join\n",
        "    )\n",
        "    tfidf = vector.fit_transform(texts)\n",
        "    dictionary = Dictionary(texts)\n",
        "    dictionary.filter_extremes(\n",
        "        no_below=3,\n",
        "        no_above=0.85,\n",
        "        keep_n=5000\n",
        "    )\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    k_topic = fine_tuning_number_topics(corpus,dictionary,texts)\n",
        "\n",
        "    terms = vector.get_feature_names()\n",
        "    result = []\n",
        "\n",
        "    nmf  = NMF(n_components = k_topic)\n",
        "    nmf.fit(tfidf)\n",
        "    for i in range(0,k_topic):\n",
        "        word_list=[]\n",
        "        for j in nmf.components_.argsort()[i,-9:-1]: # Specifies the number of words.\n",
        "            word_list.append(terms[j])\n",
        "        result.append(word_list)\n",
        "    return pd.DataFrame(result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def detect_langs_fun(x):\n",
        "    \"\"\"\n",
        "        The warper for detect_langs function will not stop the function if it returns an error.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return detect_langs(x)[0].lang\n",
        "    except:\n",
        "        return 'error'\n",
        "\n",
        "def detect_lang_croup(df:pd.Series) -> None:\n",
        "    \"\"\"\n",
        "    Verify whether the text contains any other languages besides Hebrew. We conduct a word-level check since some languages share characters.\n",
        "    In Explorer, we aggregate all languages other than Hebrew, so we are not able to perform word-level testing for those languages.\n",
        "    There is an easier solution to this problem by using regax to check the character level\n",
        "    \"\"\"\n",
        "    result = df.apply(lambda x: augment(x))\n",
        "    result = result.value_counts()\n",
        "    result = result.to_frame().reset_index()\n",
        "    return result\n",
        "\n",
        "\n",
        "def Zipf_law(df:pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculation zipf law.\n",
        "    \"\"\"\n",
        "\n",
        "    # Preparations\n",
        "    df = df.replace('[^\\u0590-\\u05fe]',' ',regex=True)\n",
        "    \n",
        "    # find freq words\n",
        "    vec = CountVectorizer(ngram_range=(1, 1)).fit(df)\n",
        "    bag_of_words = vec.transform(df)\n",
        "    dat = pd.DataFrame(zip(vec.get_feature_names(),bag_of_words.sum(axis=0).tolist()[0]))\n",
        "    dat.columns = ['words','freq']\n",
        "    dat = dat.sort_values('freq',ascending= False)\n",
        "    dat  = dat.reset_index(drop = True).reset_index(drop = False)\n",
        "    dat = dat.rename(columns = {'index':'rank'})\n",
        "    dat['rank'] = dat['rank'] + 1\n",
        "    \n",
        "    # calculate zipf law\n",
        "    dat['freq'] = dat['freq']/dat['freq'].sum()\n",
        "    dat['Zipf'] = (dat.loc[0,'freq']/dat['rank'])\n",
        "    dat['Zipf'] = dat['Zipf']/dat['Zipf'].sum()\n",
        "    \n",
        "    return dat\n",
        "\n",
        "\n",
        "def lexical_density(dirc:str):\n",
        "    \"\"\"\n",
        "    check for lexical_density. we defined list of stop words here: \n",
        "    https://github.com/NNLP-IL/Stop-Words-Hebrew\n",
        "    with the following parts of speech: DET,ADP,PRON, CCONJ, SCONJ\n",
        "    \"\"\"\n",
        "    dct_lxl = pd.read_csv(os.path.join(base_path,'Colab Notebooks','Lexical_density.txt'))\n",
        "    dct_lxl = dct_lxl['stopswords'].to_list()\n",
        "    a = os.path.join(path_save,dirc,'ngram 1.pickle')\n",
        "    with open(a, 'rb') as handle:\n",
        "        word_freq = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "    lxl = word_freq[word_freq['word'].isin(dct_lxl)]['frequency'].sum()/word_freq['frequency'].sum()\n",
        "    return lxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b4fb96",
      "metadata": {
        "scrolled": true,
        "id": "49b4fb96",
        "outputId": "6e9238b6-3109-4c0c-ffd2-8b9bfe1940c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wikipedia.csv\n",
            "ngram\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████████████████████████████████████████████████████████████████████| 316462/316462 [05:30<00:00, 958.13it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████████| 316462/316462 [06:43<00:00, 783.93it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████████| 316462/316462 [12:05<00:00, 436.06it/s]\n",
            "100%|█████████████████████████████████████████████████████████████████████████| 316462/316462 [40:08<00:00, 131.41it/s]\n",
            "100%|████████████████████████████████████████████████████████████████████████| 316462/316462 [1:02:04<00:00, 84.97it/s]\n"
          ]
        }
      ],
      "source": [
        "def save_pickle(obj,file_name,dirc):\n",
        "    import pickle\n",
        "    global path_save\n",
        "    dirc = str(dirc).split('.')[0]\n",
        "    with open(os.path.join(path_save,dirc,file_name), 'wb') as handle:\n",
        "                pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def save_parquet(obj,file_name,dirc):\n",
        "    global path_save\n",
        "    dirc = str(dirc).split('.')[0]\n",
        "    obj.to_parquet(os.path.join(path_save,dirc,file_name))\n",
        "            \n",
        "def save_function(df : pd.DataFrame,dirc : str):\n",
        "    df = df[df['line'].notnull()]\n",
        "    df['line'] = df['line'].astype('str')\n",
        "    \n",
        "    #len_word\n",
        "    print('avg_len_word')\n",
        "    temp = avg_len_word(df['line'])\n",
        "    save_pickle(temp,'len words.pickle',dirc)\n",
        "    #Zipf_law\n",
        "    print('Zipf_law')\n",
        "    temp = Zipf_law(df['line'])\n",
        "    save_pickle(temp,'Zipf law.pickle',dirc)\n",
        "    #number lines\n",
        "    print('number lines')\n",
        "    temp = number_lines(df['line'])\n",
        "    save_pickle(temp,'number lines.pickle',dirc)\n",
        "    #Character distribution\n",
        "    print('Character distribution')\n",
        "    temp = char_freq(df['line'],True)\n",
        "    save_pickle(temp,'Character distribution.pickle',dirc)\n",
        "    #ngram\n",
        "    print('ngram')\n",
        "    one_apper = 0        \n",
        "    temp = ngram_fun(df['line'],1,dirc.split('.')[0])\n",
        "    top_words = temp.pop('top unqiue words')\n",
        "    save_parquet(top_words,'ngram 1' ,dirc)\n",
        "    save_pickle(temp,'stat.pickle',dirc)\n",
        "    for x in range(2,6):\n",
        "        one_apper = 0        \n",
        "        temp = ngram_fun(df['line'],x,dirc.split('.')[0])\n",
        "        top_words = temp.pop('top unqiue words')\n",
        "        save_parquet(top_words,'ngram ' + str(x) ,dirc)\n",
        "    #identity duplicate line\n",
        "    print('identity duplicate')\n",
        "    temp = identity_duplicate_line(df['line'])\n",
        "    save_pickle(temp,'identity duplicate line.pickle',dirc)\n",
        "    #topic model\n",
        "    print('topic model')\n",
        "    import time\n",
        "    start = time.time()\n",
        "    temp = topic_model(df['line'])\n",
        "    end = time.time()\n",
        "    print(end - start)  \n",
        "    save_pickle(temp,'topic model.pickle',dirc)\n",
        "    #number word in line\n",
        "    print('number word in line')\n",
        "    temp = number_word_in_line(df['line'])\n",
        "    save_pickle(temp,'number word in line.pickle',dirc)\n",
        "    #Language recognition in corpus\n",
        "    print('Language recognition in corpus')\n",
        "    temp = detect_lang_croup(df['line'])\n",
        "    save_pickle(temp,'Language recognition in corpus.pickle',dirc)\n",
        "    #lexical_density\n",
        "    print('lexical_density')\n",
        "    temp = lexical_density(dirc.split('.')[0])\n",
        "    save_pickle(temp,'lexical_density.pickle',dirc)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "def load_database():\n",
        "    distance_plot(main_path)\n",
        "    files = np.sort(os.listdir(path_corp))\n",
        "    index = np.argwhere(files=='desktop.ini')\n",
        "    files = np.delete(files, index)\n",
        "    files = files[26:]\n",
        "\n",
        "    files_done = np.sort(os.listdir(path_save))\n",
        "    for file in files:\n",
        "        if (file.endswith('.csv') or file.endswith('.tsv')):\n",
        "            print(file)\n",
        "            df = pd.read_csv(path_corp + '/'+ file,sep = '\\t')\n",
        "            df = df.rename(columns = {'text':'line'})\n",
        "            if path_save.split(' ')[-1] == 'stanza':\n",
        "                df['line'] = df['line'].str.replace('_',' ')\n",
        "            save_function(df,file)\n",
        "\n",
        "load_database()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c3a3578",
      "metadata": {
        "id": "1c3a3578"
      },
      "source": [
        "# The next section has another version of the same function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a6d320",
      "metadata": {
        "id": "77a6d320"
      },
      "outputs": [],
      "source": [
        "def coherence_check(corpus,dictionary,index) -> int:    \n",
        "    nmf = Nmf(\n",
        "    corpus=corpus,\n",
        "    num_topics=index,\n",
        "    id2word=dictionary,\n",
        "    chunksize=1000,\n",
        "    passes=5,\n",
        "    eval_every=10,\n",
        "    minimum_probability=0,\n",
        "    random_state=0,\n",
        "    kappa=1,\n",
        ")\n",
        "    \n",
        "\n",
        "    cm = CoherenceModel(\n",
        "        model=nmf,\n",
        "        corpus=corpus,\n",
        "        coherence='u_mass'\n",
        "    )\n",
        "            \n",
        "    return round(cm.get_coherence(), 5)\n",
        "\n",
        "    \n",
        "\n",
        "def pick_best_number_topic(df:pd.DataFrame) -> int:\n",
        "    # Decide on the best topic number.\n",
        "    # In this case, we mean that it is the highest, but not higher than a small number topic by 1.1.\n",
        "    df['shift'] = df['num'].shift(1)\n",
        "    df['shift'] = df['shift'] > df['num']\n",
        "    df['shift'] = df['shift'].cumsum()\n",
        "    df = df[df.index == df['shift']]\n",
        "\n",
        "    df['shift'] = df['score'].shift(1)\n",
        "    df['shift'] = df['shift']/df['score']\n",
        "    df['improve 10%'] = df['shift'] > 1.1\n",
        "    df['temp'] = df['improve 10%'].cumsum()\n",
        "    best_num_topics = df[df['temp'] == df['temp'].min()].tail(1)['num'].item()\n",
        "    return best_num_topics\n",
        "\n",
        "\n",
        "def fine_tuning_number_topics(corpus,dictionary) -> int:\n",
        "    topic_nums = list(np.arange(5, 45 + 1, 5))\n",
        "    coherence_scores = []\n",
        "    for index in topic_nums:\n",
        "        print(index)\n",
        "        temp = coherence_check(corpus,dictionary,index)\n",
        "        coherence_scores.append(temp)\n",
        "\n",
        "    \n",
        "\n",
        "    scores = list(zip(topic_nums, coherence_scores))\n",
        "    temp = pd.DataFrame(sorted(scores, key=itemgetter(1), reverse=True),columns= ['num','score'])\n",
        "    k = pick_best_number_topic(temp)\n",
        "    return k\n",
        "\n",
        "\n",
        "\n",
        "def topic_model(df:pd.Series):\n",
        "    # prefom NMF  topic analysis\n",
        "    documents = [doc.split() for doc in df]\n",
        "    dictionary = Dictionary(documents)\n",
        "    dictionary.filter_extremes(no_below=5, no_above=0.85, keep_n=20000) \n",
        "\n",
        "    \n",
        "    tfidf = TfidfModel(dictionary=dictionary)\n",
        "\n",
        "    corpus = [\n",
        "        dictionary.doc2bow(document)\n",
        "        for document\n",
        "        in documents\n",
        "    ]\n",
        "\n",
        "    corpus_tfidf = list(tfidf[corpus])\n",
        "\n",
        "    k_topic = fine_tuning_number_topics(corpus_tfidf,dictionary)\n",
        "    nmf = Nmf(\n",
        "    corpus=corpus,\n",
        "    num_topics=k_topic,\n",
        "    id2word=dictionary,\n",
        "    chunksize=1000,\n",
        "    passes=5,\n",
        "    eval_every=10,\n",
        "    minimum_probability=0,\n",
        "    random_state=0,\n",
        "    kappa=1,\n",
        ")\n",
        "\n",
        "    topics = nmf.show_topics()\n",
        "    topics = pd.DataFrame([ re.findall('\"([^\"]*)\"', i[1]) for i in topics])\n",
        "    return nmf\n",
        "from gensim.models import CoherenceModel, LdaModel, TfidfModel\n",
        "\n",
        "temp = topic_model(df['line'])\n",
        "temp.show_topics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44894400",
      "metadata": {
        "id": "44894400"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import common_texts\n",
        "common_dictionary = Dictionary(common_texts)\n",
        "common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]\n",
        "nmf = Nmf(common_corpus, num_topics=20)\n",
        "len(nmf.show_topics()) # number of topics dont match that number that request!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
      }
    },
    "colab": {
      "name": "function.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}