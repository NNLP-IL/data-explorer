{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WebiksInc/data-explorer/blob/main/function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U gensim\n",
        "!pip install langdetect\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGAAPyPqfvgk",
        "outputId": "a6e5b3e7-6bb0-4aa6-eeda-235ab3eb9aec"
      },
      "id": "CGAAPyPqfvgk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (4.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xuOxcOBD_uPn",
      "metadata": {
        "id": "xuOxcOBD_uPn"
      },
      "source": [
        "# Import libraries, and utility function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f385394a",
      "metadata": {
        "id": "f385394a"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from scipy.special import rel_entr\n",
        "\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.nmf import Nmf\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "from langdetect import detect_langs\n",
        "from operator import itemgetter\n",
        "import itertools\n",
        "import re\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_rows\", 500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PQqD9aql__88",
      "metadata": {
        "id": "PQqD9aql__88"
      },
      "outputs": [],
      "source": [
        "# Utility function\n",
        "\n",
        "\n",
        "def load_parquet_plot_distance(dirc: str, file: str):\n",
        "    path = os.path.join(base_path, \"corpus\", \"without stopwords\", dirc, file)\n",
        "    return pd.read_parquet(path)\n",
        "\n",
        "\n",
        "def remove_niqqud_from_string(my_string):\n",
        "    return \"\".join([\"\" if 1456 <= ord(c) <= 1479 else c for c in my_string])\n",
        "\n",
        "\n",
        "def save_pickle(obj, file_name, dirc):\n",
        "    global path_save\n",
        "    dirc = str(dirc).split(\".\")[0]\n",
        "    with open(os.path.join(path_save, dirc, file_name), \"wb\") as handle:\n",
        "        pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "def save_parquet(obj, file_name, dirc):\n",
        "    global path_save\n",
        "    dirc = str(dirc).split(\".\")[0]\n",
        "    obj.to_parquet(os.path.join(path_save, dirc, file_name))\n",
        "\n",
        "\n",
        "def update_word_dict(line, n, words):\n",
        "    # Update the word dict for count ngram on the corpus.\n",
        "    line = line.split(\" \")\n",
        "    line = [i for i in line if (i != '\"') & (len(i) > 0)]\n",
        "    line = list(map(remove_niqqud_from_string, line))\n",
        "    n_grams = zip(*(line[i:] for i in range(n)))\n",
        "    temp = Counter(n_grams)\n",
        "    words.update(temp)\n",
        "\n",
        "\n",
        "def detect_langs_fun(x):\n",
        "    # The warper for detect_langs function will not stop the function if it returns an error.\n",
        "    try:\n",
        "        return detect_langs(x)[0].lang\n",
        "    except:\n",
        "        return \"error\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a14ZLwBI_3Xn",
      "metadata": {
        "id": "a14ZLwBI_3Xn"
      },
      "source": [
        "# main code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "re_nVjMkeECW",
      "metadata": {
        "id": "re_nVjMkeECW"
      },
      "outputs": [],
      "source": [
        "# Define path\n",
        "\n",
        "# base_path = os.path.join('C:',os.sep,'Users','user','My Drive (roei_shlezinger@webiks.com)')\n",
        "base_path = os.path.join(\"/content\", \"drive\", \"Shareddrives\",\"National NLP Github\",\"Explorer\")\n",
        "# /content/drive/Shareddrives/National NLP Github/Explorer\n",
        "\n",
        "# with stopwords without stanza\n",
        "path_corp = os.path.join(base_path, \"work\", \"with stopwords\")\n",
        "path_save = os.path.join(base_path, \"corpus\", \"with stopwords\")\n",
        "\n",
        "# without stopwords without stanza\n",
        "path_corp = os.path.join(base_path, \"work\", \"without stopwords\")\n",
        "path_save = os.path.join(base_path, \"corpus\", \"without stopwords\")\n",
        "\n",
        "# without stopwords with stanza\n",
        "path_corp = os.path.join(base_path, \"work\", \"without stopwords and stanza\")\n",
        "path_save = os.path.join(base_path, \"corpus\", \"without stopwords and stanza\")\n",
        "\n",
        "# with stopwords with stanza\n",
        "path_corp = os.path.join(base_path, \"work\", \"with stopwords and stanza\")\n",
        "path_save = os.path.join(base_path, \"corpus\", \"with stopwords and stanza\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ba7fd93",
      "metadata": {
        "id": "8ba7fd93",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# function distance plot\n",
        "\n",
        "\n",
        "def lng_distubtion_function(dis_word1, dis_word2) -> float:\n",
        "    n = 5000  # tunable parameter. the number of words to keep\n",
        "\n",
        "    df_a = dis_word1[\n",
        "        (dis_word1[\"word\"].isin(dis_word1[\"word\"].head(n)))\n",
        "        | (dis_word1[\"word\"].isin(dis_word2[\"word\"].head(n)))\n",
        "    ]\n",
        "    df_b = dis_word2[\n",
        "        (dis_word2[\"word\"].isin(dis_word2[\"word\"].head(n)))\n",
        "        | (dis_word2[\"word\"].isin(dis_word1[\"word\"].head(n)))\n",
        "    ]\n",
        "    df_a = df_a[df_a[\"word\"].isin(df_b[\"word\"])]\n",
        "    df_b = df_b[df_b[\"word\"].isin(df_a[\"word\"])]\n",
        "\n",
        "    df_a = df_a.sort_values(\"word\")[\"frequency\"]\n",
        "    df_b = df_b.sort_values(\"word\")[\"frequency\"]\n",
        "    return sum(rel_entr(list(df_a), list(df_b))) + sum(rel_entr(list(df_b), list(df_a)))\n",
        "\n",
        "\n",
        "def distance_plot(main_path):\n",
        "    # Calculates the distance between each pair of corpuses\n",
        "\n",
        "    corpus = np.sort(os.listdir(os.path.join(main_path, \"corpus\", \"without stopwords\")))\n",
        "    corpus = [x.split(\".\")[0] for x in corpus]  # remove csv ending\n",
        "\n",
        "    result = pd.DataFrame(index=corpus, columns=corpus)\n",
        "    for index in itertools.combinations(corpus, 2):\n",
        "        dis_word1 = load_parquet_plot_distance(index[0], \"ngram 1\")\n",
        "        dis_word1[\"frequency\"] = dis_word1[\"frequency\"] / dis_word1[\"frequency\"].sum()\n",
        "        dis_word2 = load_parquet_plot_distance(index[1], \"ngram 1\")\n",
        "        dis_word2[\"frequency\"] = dis_word2[\"frequency\"] / dis_word2[\"frequency\"].sum()\n",
        "        result.loc[index[0], index[1]] = lng_distubtion_function(dis_word1, dis_word2)\n",
        "    with open(\n",
        "        os.path.join(base_path, \"corpus\", \"main\", \"distance_plot.pickle\"), \"wb\"\n",
        "    ) as handle:\n",
        "        pickle.dump(result, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd748978",
      "metadata": {
        "id": "dd748978"
      },
      "outputs": [],
      "source": [
        "# Function process data\n",
        "\n",
        "\n",
        "def number_lines(df: pd.Series) -> int:\n",
        "    num_line = df.shape[0]\n",
        "    return num_line\n",
        "\n",
        "\n",
        "def len_word_in_line(line: str) -> list:\n",
        "    # The length of each word in the line\n",
        "    line = line.replace(\".\", \" \")\n",
        "    line = \" \".join(line.split())  # remove multiple spaces\n",
        "    line = line.split(\" \")\n",
        "    return [len(i) for i in line]\n",
        "\n",
        "\n",
        "def avg_len_word(df: pd.Series) -> int:\n",
        "    # Return the average length of all words in the series\n",
        "    lines = df.apply(lambda x: len_word_in_line(x))\n",
        "    avg_word = np.mean([x for xs in lines for x in xs])\n",
        "    return avg_word\n",
        "\n",
        "\n",
        "def number_word_in_line(df: pd.Series) -> dict:\n",
        "    df = df.apply(lambda x: \" \".join(x.split()))  # remove multiple spaces\n",
        "    result = df.apply(lambda x: len(x.split(\" \")))\n",
        "    return {\"mean\": np.round(result.mean(), 2), \"median\": result.median()}\n",
        "\n",
        "\n",
        "def char_freq(df: pd.Series) -> dict:\n",
        "    # Distribution of all characters in the corpus\n",
        "    x = df.tolist()\n",
        "    x = Counter(\"\".join(x))\n",
        "    x = dict(sorted(x.items(), key=lambda item: item[1]))\n",
        "    return x\n",
        "\n",
        "\n",
        "def lexical_density(dirc: str):\n",
        "    # check for lexical_density. we defined list of stop words here:\n",
        "    # https://github.com/NNLP-IL/Stop-Words-Hebrew\n",
        "    # with the following parts of speech: DET,ADP,PRON, CCONJ, SCONJ\n",
        "    dct_lxl = pd.read_csv(\n",
        "        os.path.join(base_path, \"Colab Notebooks\", \"Lexical_density.txt\")\n",
        "    )\n",
        "    dct_lxl = dct_lxl[\"stopswords\"].to_list()\n",
        "    a = os.path.join(path_save, dirc, \"ngram 1.pickle\")\n",
        "    path_parquet = os.path.join(path_save, dirc, \"ngram 1\")\n",
        "    word_freq = pd.read_parquet(path_parquet)\n",
        "    lxl = (\n",
        "        word_freq[word_freq[\"word\"].isin(dct_lxl)][\"frequency\"].sum()\n",
        "        / word_freq[\"frequency\"].sum()\n",
        "    )\n",
        "    return lxl\n",
        "\n",
        "\n",
        "def identity_duplicate_line(df: pd.Series) -> int:\n",
        "    return df.duplicated().sum()\n",
        "\n",
        "\n",
        "def detect_lang_croup(df: pd.Series) -> None:\n",
        "    # Verify whether the text contains any other languages besides Hebrew. We conduct a word-level check since some languages share characters.\n",
        "    # In Explorer, we aggregate all languages other than Hebrew, so we are not able to perform word-level testing for those languages.\n",
        "    # There is an easier solution to this problem by using regax to check the character level\n",
        "    result = df.apply(lambda x: detect_langs_fun(x))\n",
        "    result = result.value_counts()\n",
        "    result = result.to_frame().reset_index()\n",
        "    return result\n",
        "\n",
        "\n",
        "def Zipf_law(df: pd.Series) -> pd.DataFrame:\n",
        "    # Calculation zipf law.\n",
        "\n",
        "    # Preparations\n",
        "    df = df.replace(\"[^\\u0590-\\u05fe]\", \" \", regex=True)\n",
        "\n",
        "    # find freq words\n",
        "    vec = CountVectorizer(ngram_range=(1, 1)).fit(df)\n",
        "    bag_of_words = vec.transform(df)\n",
        "    dat = pd.DataFrame(\n",
        "        zip(vec.get_feature_names(), bag_of_words.sum(axis=0).tolist()[0])\n",
        "    )\n",
        "    dat.columns = [\"words\", \"freq\"]\n",
        "    dat = dat.sort_values(\"freq\", ascending=False)\n",
        "    dat = dat.reset_index(drop=True).reset_index(drop=False)\n",
        "    dat = dat.rename(columns={\"index\": \"rank\"})\n",
        "    dat[\"rank\"] = dat[\"rank\"] + 1\n",
        "\n",
        "    # calculate zipf law\n",
        "    dat[\"freq\"] = dat[\"freq\"] / dat[\"freq\"].sum()\n",
        "    dat[\"Zipf\"] = dat.loc[0, \"freq\"] / dat[\"rank\"]\n",
        "    dat[\"Zipf\"] = dat[\"Zipf\"] / dat[\"Zipf\"].sum()\n",
        "\n",
        "    return dat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hDYR_FfzYUS-",
      "metadata": {
        "id": "hDYR_FfzYUS-"
      },
      "outputs": [],
      "source": [
        "# Extract topics\n",
        "def coherence_check(corpus, index, dictionary, texts) -> int:\n",
        "    nmf = Nmf(\n",
        "        corpus=corpus,\n",
        "        num_topics=index,\n",
        "        id2word=dictionary,\n",
        "        chunksize=1000,\n",
        "        passes=5,\n",
        "        kappa=0.1,\n",
        "        minimum_probability=0.01,\n",
        "        w_max_iter=300,\n",
        "        w_stop_condition=0.0001,\n",
        "        h_max_iter=100,\n",
        "        h_stop_condition=0.001,\n",
        "        eval_every=10,\n",
        "        normalize=True,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    cm = CoherenceModel(model=nmf, texts=texts, dictionary=dictionary, coherence=\"c_v\")\n",
        "    return round(cm.get_coherence(), 5)\n",
        "\n",
        "\n",
        "def pick_best_number_topic(df: pd.DataFrame) -> int:\n",
        "    # Decide on the best topic number.\n",
        "    # In this case, we mean that it is the highest, but not higher than a small number topic by 1.1.\n",
        "    df[\"shift\"] = df[\"num\"].shift(1)\n",
        "    df[\"shift\"] = df[\"shift\"] > df[\"num\"]\n",
        "    df[\"shift\"] = df[\"shift\"].cumsum()\n",
        "    df = df[df.index == df[\"shift\"]]\n",
        "\n",
        "    df[\"shift\"] = df[\"score\"].shift(1)\n",
        "    df[\"shift\"] = df[\"shift\"] / df[\"score\"]\n",
        "    df[\"improve 10%\"] = df[\"shift\"] > 1.1\n",
        "    df[\"temp\"] = df[\"improve 10%\"].cumsum()\n",
        "    best_num_topics = df[df[\"temp\"] == df[\"temp\"].min()].tail(1)[\"num\"].item()\n",
        "    return best_num_topics\n",
        "\n",
        "\n",
        "def fine_tuning_number_topics(corpus, dictionary, texts) -> int:\n",
        "    topic_nums = list(np.arange(5, 45 + 1, 5))\n",
        "    coherence_scores = []\n",
        "    for index in topic_nums:\n",
        "        print(index)\n",
        "        temp = coherence_check(corpus, index, dictionary, texts)\n",
        "        coherence_scores.append(temp)\n",
        "\n",
        "    scores = list(zip(topic_nums, coherence_scores))\n",
        "    temp = pd.DataFrame(\n",
        "        sorted(scores, key=itemgetter(1), reverse=True), columns=[\"num\", \"score\"]\n",
        "    )\n",
        "    k = pick_best_number_topic(temp)\n",
        "    print(k)\n",
        "    return k\n",
        "\n",
        "\n",
        "def topic_model(df: pd.Series):\n",
        "    # prefom NMF  topic analysis\n",
        "    texts = df.apply(lambda x: x.split(\" \"))\n",
        "\n",
        "    vector = TfidfVectorizer(\n",
        "        min_df=3,\n",
        "        max_df=0.85,\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        preprocessor=\" \".join,\n",
        "    )\n",
        "    tfidf = vector.fit_transform(texts)\n",
        "    dictionary = Dictionary(texts)\n",
        "    dictionary.filter_extremes(no_below=3, no_above=0.85, keep_n=5000)\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    k_topic = fine_tuning_number_topics(corpus, dictionary, texts)\n",
        "\n",
        "    terms = vector.get_feature_names()\n",
        "    result = []\n",
        "\n",
        "    nmf = NMF(n_components=k_topic)\n",
        "    nmf.fit(tfidf)\n",
        "    for i in range(0, k_topic):\n",
        "        word_list = []\n",
        "        for j in nmf.components_.argsort()[i, -9:-1]:  # Specifies the number of words.\n",
        "            word_list.append(terms[j])\n",
        "        result.append(word_list)\n",
        "    return pd.DataFrame(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bPD8nvbeYLKc",
      "metadata": {
        "id": "bPD8nvbeYLKc"
      },
      "outputs": [],
      "source": [
        "# Ngram function\n",
        "def calculation_gini_index(dat: pd.Series) -> int:\n",
        "    sorted_x = np.sort(dat)\n",
        "    n = len(sorted_x)\n",
        "    cumx = np.cumsum(sorted_x, dtype=float)\n",
        "    gini = (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
        "    return gini\n",
        "\n",
        "\n",
        "def ngram_fun(df: pd.Series, ngram: int, name: str) -> dict:\n",
        "    # Custom function to ngram. We created a function to address the memory issue that arises in large databases.\n",
        "    # By using our function, you are able to go line by line without having to worry about memory issues\n",
        "    # We also delete all words that appear one time. Most of it is junk, and it takes a great deal of memory.\n",
        "\n",
        "    # Preparations\n",
        "    df = df.replace('[^\\u0590-\\u05fe\"]', \" \", regex=True)  # keep only hebrew character\n",
        "    # df = df.str.replace('[^\\w\\s\"]',' ') # remove punctuation\n",
        "\n",
        "    # main code\n",
        "    words = Counter()  # create dictionary\n",
        "    not_important = df.astype(str).progress_apply(\n",
        "        lambda x: update_word_dict(x, ngram, words)\n",
        "    )  # update the dictionary\n",
        "    full_size = len(words)\n",
        "    # return words\n",
        "    ngram_dict = {\n",
        "        \" \".join(k): v for k, v in words.items() if v != 1\n",
        "    }  # keep only appear twice or more\n",
        "    word_one_apper_shape = full_size - len(ngram_dict)\n",
        "\n",
        "    ngram_df = pd.DataFrame(ngram_dict.items(), columns=[\"word\", \"frequency\"])\n",
        "    ngram_df = ngram_df.sort_values(\"frequency\", ascending=False)\n",
        "\n",
        "    # gini index\n",
        "    gini = calculation_gini_index(ngram_df[\"frequency\"])\n",
        "\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"shape unique words\": word_one_apper_shape + ngram_df.shape[0],\n",
        "        \"shape appeared once\": word_one_apper_shape,\n",
        "        \"Percent appeared once\": word_one_apper_shape\n",
        "        / (word_one_apper_shape + ngram_df.shape[0]),\n",
        "        \"top unqiue words\": ngram_df,\n",
        "        \"type-token ratio\": (word_one_apper_shape + ngram_df.shape[0])\n",
        "        / (word_one_apper_shape + ngram_df[\"frequency\"].sum()),\n",
        "        \"Number Of Words\": (word_one_apper_shape + ngram_df[\"frequency\"].sum()),\n",
        "        \"gini\": gini,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49b4fb96",
      "metadata": {
        "id": "49b4fb96",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Runs all the above functions, and saves the results\n",
        "\n",
        "\n",
        "def save_function(df: pd.DataFrame, dirc: str):\n",
        "    df = df[df[\"line\"].notnull()]\n",
        "    df[\"line\"] = df[\"line\"].astype(\"str\")\n",
        "\n",
        "    # len_word\n",
        "    print(\"avg_len_word\")\n",
        "    temp = avg_len_word(df[\"line\"])\n",
        "    save_pickle(temp, \"len words.pickle\", dirc)\n",
        "    # Zipf_law\n",
        "    print(\"Zipf_law\")\n",
        "    temp = Zipf_law(df[\"line\"])\n",
        "    save_pickle(temp, \"Zipf law.pickle\", dirc)\n",
        "    # number lines\n",
        "    print(\"number lines\")\n",
        "    temp = number_lines(df[\"line\"])\n",
        "    save_pickle(temp, \"number lines.pickle\", dirc)\n",
        "    # Character distribution\n",
        "    print(\"Character distribution\")\n",
        "    temp = char_freq(df[\"line\"])\n",
        "    save_pickle(temp, \"Character distribution.pickle\", dirc)\n",
        "    # ngram\n",
        "    print(\"ngram\")\n",
        "    temp = ngram_fun(df[\"line\"], 1, dirc.split(\".\")[0])\n",
        "    top_words = temp.pop(\"top unqiue words\")\n",
        "    save_parquet(top_words, \"ngram 1\", dirc)\n",
        "    save_pickle(temp, \"stat.pickle\", dirc)\n",
        "    for x in range(2, 6):\n",
        "        temp = ngram_fun(df[\"line\"], x, dirc.split(\".\")[0])\n",
        "        top_words = temp.pop(\"top unqiue words\")\n",
        "        save_parquet(top_words, \"ngram \" + str(x), dirc)\n",
        "    # identity duplicate line\n",
        "    print(\"identity duplicate\")\n",
        "    temp = identity_duplicate_line(df[\"line\"])\n",
        "    save_pickle(temp, \"identity duplicate line.pickle\", dirc)\n",
        "    # topic model\n",
        "    print(\"topic model\")\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "    temp = topic_model(df[\"line\"])\n",
        "    end = time.time()\n",
        "    print(end - start)\n",
        "    save_pickle(temp, \"topic model.pickle\", dirc)\n",
        "    # number word in line\n",
        "    print(\"number word in line\")\n",
        "    temp = number_word_in_line(df[\"line\"])\n",
        "    save_pickle(temp, \"number word in line.pickle\", dirc)\n",
        "    # Language recognition in corpus\n",
        "    print(\"Language recognition in corpus\")\n",
        "    temp = detect_lang_croup(df[\"line\"])\n",
        "    save_pickle(temp, \"Language recognition in corpus.pickle\", dirc)\n",
        "    # lexical_density\n",
        "    print(\"lexical_density\")\n",
        "    temp = lexical_density(dirc.split(\".\")[0])\n",
        "    save_pickle(temp, \"lexical_density.pickle\", dirc)\n",
        "\n",
        "\n",
        "def load_database():\n",
        "\n",
        "    files = np.sort(os.listdir(path_corp))\n",
        "    index = np.argwhere(files == \"desktop.ini\")\n",
        "    files = np.delete(files, index)\n",
        "\n",
        "    files_done = np.sort(os.listdir(path_save))\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\") or file.endswith(\".tsv\"):\n",
        "            print(file)\n",
        "            df = pd.read_csv(os.path.join(path_corp, file), sep=\"\\t\")[\n",
        "                [\"line_new\"]\n",
        "            ]\n",
        "            df = df.rename(columns={\"line_new\": \"line\"})\n",
        "\n",
        "            save_function(df, file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "function.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}