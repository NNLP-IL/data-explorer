{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WebiksInc/data-explorer/blob/add-stop-word/function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "hidden": true
              }
            }
          }
        },
        "id": "O8ZAWclXi7Oa",
        "outputId": "607eeded-e7e0-432e-caf7-3d4829d8efeb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "import pickle\n",
        "\n",
        "pd.set_option('display.max_rows', 500)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import IPython.display\n",
        "from IPython.display import display, clear_output\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# main_path = r'C:/Users/user/My Drive (roei_shlezinger@webiks.com)'\n",
        "main_path = r'/content/drive/MyDrive'\n",
        "# path_corp = main_path + r'/corpus/with stopwords'\n",
        "# path_main = main_path + r'/corpus/for display/with stopwords'\n",
        "\n",
        "path_corp = main_path + r'/corpus/without stopwords'\n",
        "path_main = main_path + r'/corpus/for display/without stopwords'\n",
        "\n",
        "process_data = False\n",
        "\n",
        "!pip install langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkjgxqMt6MpY",
        "outputId": "f30437ae-b40b-4037-b6e4-bbc49e96af1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trm-QeZQrkpE"
      },
      "source": [
        "# NLP IL Hebrew comparisons Dataset Explorer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "col": 0,
                "height": 5,
                "row": 0,
                "width": 4
              }
            }
          }
        },
        "id": "NNl_xmaai7Of"
      },
      "source": [
        "## competitive analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "col": 4,
                "height": 12,
                "row": 0,
                "width": 8
              }
            }
          }
        },
        "id": "YhyFGPwdi7On",
        "outputId": "05bbb630-f63a-481f-dd07-fae335aa7a2f",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"56f4e719-dde4-4aef-ab3b-8bd7c9d03f41\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"56f4e719-dde4-4aef-ab3b-8bd7c9d03f41\")) {                    Plotly.newPlot(                        \"56f4e719-dde4-4aef-ab3b-8bd7c9d03f41\",                        [{\"orientation\":\"h\",\"x\":[1814295,10943335,41809752,68991,232695,69127,514621,11404027,8229027,20152,189416,8384062,576567,75969,19446,191334,236690,179135,807627,1191687,585858,124759,715140,134842745],\"y\":[\"Arutz 7 artical\",\"Arutz 7 news\",\"ben yehuda\",\"bible\",\"doctors\",\"foodpage\",\"foodWalla\",\"haaretz\",\"HeBERT\",\"Hebrew Dotted Text\",\"infomad\",\"knesset\",\"learningMan (mila)\",\"Nature of Healing\",\"paraShoot\",\"sentiment_Data\",\"spokenHebrew (mila)\",\"sport5\",\"sportWalla\",\"tapuz\",\"themarker\",\"To Be Healthy articles\",\"To Be Healthy forum\",\"wiki\"],\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"bgcolor\":\"#E5ECF6\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"zerolinewidth\":2}}},\"xaxis\":{\"type\":\"log\"},\"title\":{\"text\":\"Number of Words (log)\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('56f4e719-dde4-4aef-ab3b-8bd7c9d03f41');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def number_words(df:pd.Series) -> int:\n",
        "    return df[df.notnull()].astype(\"string\").str.split().apply(len).sum()\n",
        "\n",
        "def set_cd(dirc):       \n",
        "    create_folder(path_main + '/' + dirc)\n",
        "    os.chdir(path_main + '/' + dirc)\n",
        "\n",
        "def create_folder(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "        \n",
        "def plot_shape_all_data():\n",
        "    if process_data:\n",
        "        result = {}\n",
        "        fx = lambda x: [number_words(x)]\n",
        "        for root, dirs, files in os.walk(path_corp):\n",
        "            for file in files:\n",
        "                if (file.endswith('.csv') or file.endswith('.tsv')):\n",
        "                    print(file)\n",
        "                    df = pd.read_csv(path_corp + '/'+ file,sep = '\\t')\n",
        "                    df = df.rename(columns = {'text':'line'})\n",
        "                    result[file.split('.')[0]] = fx(df['line'])\n",
        "        result = pd.DataFrame.from_dict(result, orient='index').reset_index()\n",
        "        result.columns = ['Name Database','Number of Words']\n",
        "        result.to_csv(main_path + '/corpus/for display/main/result.csv')\n",
        "    else:\n",
        "        result = pd.read_csv(main_path + '/corpus/for display/main/result.csv')\n",
        "        fig = go.Figure(go.Bar(x=result['Number of Words'], y=result['Name Database'],orientation='h'))\n",
        "        fig.update_xaxes(type=\"log\")\n",
        "        fig.update_layout(showlegend=False,\n",
        "                         title = {'text': \"Number of Words (log)\"})\n",
        "        fig.show()\n",
        "process_data = False\n",
        "# process_data = True\n",
        "plot_shape_all_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "hidden": true
              }
            }
          }
        },
        "id": "CCKixAC0i7Op",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# function\n",
        "\n",
        "def number_lines(df:pd.Series) -> int:\n",
        "    num_line = df.shape[0]\n",
        "    return num_line\n",
        "\n",
        "def Connecting_words(x):\n",
        "    x = x.replace(\".\", \" \")\n",
        "    x = ' '.join(x.split())\n",
        "    x = x.split(' ')\n",
        "    return [len(i) for i in x]\n",
        "\n",
        "def len_word(df:pd.Series):\n",
        "    temp = df.apply(lambda x: Connecting_words(x))\n",
        "    temp = np.mean([x for xs in temp for x in xs])\n",
        "    return temp\n",
        "\n",
        "\n",
        "def number_word_in_line(df:pd.Series) -> dict:\n",
        "    result = df.apply(lambda x: len(x.split(' ')))\n",
        "    # fig = px.histogram(result, x=\"line\")\n",
        "    return {'mean' : np.round(result.mean()),\n",
        "            'median' : result.median()}\n",
        "\n",
        "\n",
        "def char_freq(df:pd.Series,plot = False) -> px.bar:\n",
        "    from collections import Counter\n",
        "    x = df.tolist()\n",
        "    x = Counter(''.join(x))\n",
        "    x = dict(sorted(x.items(), key=lambda item: item[1]))\n",
        "    x.pop('\\u200f',None) #TODO: it's ok to delete this char?\n",
        "    return x\n",
        "    if plot:\n",
        "        fig = px.bar(x=x.values(), y=x.keys(), height=1500,width=800)\n",
        "        return fig\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "def ngram(df:pd.Series, ngram:int) -> dict:\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    df = df.replace('[^\\u0590-\\u05fe a-zA-Z]',' ',regex=True)\n",
        "    # Preparations\n",
        "    vec = CountVectorizer(ngram_range=(ngram, ngram)).fit(df)\n",
        "    bag_of_words = vec.transform(df)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    dat = pd.DataFrame(words_freq, columns=['word','frequency'])\n",
        "    dat = dat.sort_values('frequency',ascending = False)\n",
        "    one_apper = dat[dat['frequency'] == 1]\n",
        "\n",
        "    #gini\n",
        "    sorted_x = np.sort(dat['frequency'])\n",
        "    n = len(sorted_x)\n",
        "    cumx = np.cumsum(sorted_x, dtype=float)\n",
        "    gini = ((n + 1 - 2 * np.sum(cumx) / cumx[-1])/ n)\n",
        "\n",
        "    return({'shape unique words' : dat.shape[0],\n",
        "                'shape appeared once' : one_apper.shape[0],\n",
        "                'Percent appeared once' : one_apper.shape[0]/dat.shape[0],\n",
        "                'top unqiue words' : dat,\n",
        "#                 'tail unique words' : one_apper.sample(number_return),\n",
        "                'type-token ratio' : dat.shape[0]/dat['frequency'].sum(),\n",
        "                'Number Of Words' : dat['frequency'].sum(),\n",
        "                'gini' : gini\n",
        "                }\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "def identity_duplicate_line(df:pd.Series) -> int:\n",
        "    return df.duplicated().sum()\n",
        "\n",
        "def topic_model(df:pd.Series):\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    texts = df.apply(lambda x : x.split(' '))\n",
        "    vector = TfidfVectorizer(\n",
        "        min_df=3,\n",
        "        max_df=0.85,\n",
        "        max_features=5000,\n",
        "        ngram_range=(1, 2),\n",
        "        preprocessor=' '.join\n",
        "    )\n",
        "    tfidf = vector.fit_transform(texts)\n",
        "    \n",
        "    from gensim.corpora import Dictionary\n",
        "    from sklearn.decomposition import NMF\n",
        "    from gensim.models.nmf import Nmf\n",
        "    from gensim.models.coherencemodel import CoherenceModel\n",
        "\n",
        "    dictionary = Dictionary(texts)\n",
        "\n",
        "\n",
        "    dictionary.filter_extremes(\n",
        "        no_below=3,\n",
        "        no_above=0.85,\n",
        "        keep_n=5000\n",
        "    )\n",
        "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "\n",
        "    # Create a list of the topic numbers we want to try\n",
        "    topic_nums = list(np.arange(5, 45 + 1, 5))\n",
        "\n",
        "    # Run the nmf model and calculate the coherence score\n",
        "    # for each number of topics\n",
        "    coherence_scores = []\n",
        "\n",
        "    for num in topic_nums:\n",
        "        print(num)\n",
        "        nmf = Nmf(\n",
        "            corpus=corpus,\n",
        "            num_topics=num,\n",
        "            id2word=dictionary,\n",
        "            chunksize=2000,\n",
        "            passes=5,\n",
        "            kappa=.1,\n",
        "            minimum_probability=0.01,\n",
        "            w_max_iter=300,\n",
        "            w_stop_condition=0.0001,\n",
        "            h_max_iter=100,\n",
        "            h_stop_condition=0.001,\n",
        "            eval_every=10,\n",
        "            normalize=True,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Run the coherence model to get the score\n",
        "        cm = CoherenceModel(\n",
        "            model=nmf,\n",
        "            texts=texts,\n",
        "            dictionary=dictionary,\n",
        "            coherence='c_v'\n",
        "        )\n",
        "\n",
        "        coherence_scores.append(round(cm.get_coherence(), 5))\n",
        "\n",
        "    from operator import itemgetter\n",
        "\n",
        "    scores = list(zip(topic_nums, coherence_scores))\n",
        "    temp = pd.DataFrame(sorted(scores, key=itemgetter(1), reverse=True))\n",
        "    temp['shift'] = temp[0].shift(1)\n",
        "    temp['shift'] = temp['shift'] > temp[0]\n",
        "    temp['shift'] = temp['shift'].cumsum()\n",
        "    temp = temp[temp.index == temp['shift']]\n",
        "\n",
        "    temp['shift'] = temp[1].shift(1)\n",
        "    temp['shift'] = temp['shift']/temp[1]\n",
        "    temp['improve 10%'] = temp['shift'] > 1.1\n",
        "    temp['temp'] = temp['improve 10%'].cumsum()\n",
        "    best_num_topics = temp[temp['temp'] == temp['temp'].min()].tail(1)[0].item()\n",
        "    \n",
        "    terms = vector.get_feature_names()\n",
        "    result = []\n",
        "    k = best_num_topics\n",
        "    nmf  = NMF(n_components = k)\n",
        "    nmf.fit(tfidf)\n",
        "    for i in range(0,k):\n",
        "        word_list=[]\n",
        "        # print(\"Topic%d:\"% i)\n",
        "        for j in nmf.components_.argsort()[i,-9:-1]:\n",
        "            word_list.append(terms[j])\n",
        "        result.append(word_list)\n",
        "    return pd.DataFrame(result)\n",
        "\n",
        "def topic_model_old(df:pd.Series,k = 4) -> list:\n",
        "    # https://github.com/ashishsalunkhe/Topic-Modeling-using-LDA-and-K-Means-Clustering/blob/master/newsgroup.ipynb\n",
        "    #important notebook\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    text_content = df\n",
        "    vector = TfidfVectorizer()\n",
        "    tfidf = vector.fit_transform(text_content)\n",
        "\n",
        "    terms = vector.get_feature_names()\n",
        "\n",
        "    from sklearn.decomposition import NMF\n",
        "    result = []\n",
        "\n",
        "    nmf  = NMF(n_components = k)\n",
        "    nmf.fit(tfidf)\n",
        "    for i in range(0,k):\n",
        "        word_list=[]\n",
        "        # print(\"Topic%d:\"% i)\n",
        "        for j in nmf.components_.argsort()[i,-16:-1]:\n",
        "            word_list.append(terms[j])\n",
        "        result.append(word_list)\n",
        "    return result\n",
        "\n",
        "def co_occurrence(df:pd.Series,dirc: str) -> pd.DataFrame:\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "    temp = df.apply(lambda x: x.split('.')).explode()\n",
        "    count_model = CountVectorizer(ngram_range=(1,1))\n",
        "    X = count_model.fit_transform(temp)\n",
        "    Xc = (X.T * X)\n",
        "    Xc.setdiag(0)\n",
        "    co_oc = pd.DataFrame(Xc.todense(), \n",
        "                        columns=count_model.get_feature_names(), \n",
        "                        index=count_model.get_feature_names())\n",
        "    word_co = load_pickle(dirc,'ngram 1.pickle')['top unqiue words']['word'].head().to_list()\n",
        "    result = {}\n",
        "    for word in word_co:\n",
        "        temp = co_oc[word].nlargest(10).to_frame().reset_index()\n",
        "        temp.columns = ['number','word']\n",
        "        temp = temp.sort_values('word')\n",
        "        result[word] = temp\n",
        "    return result\n",
        "\n",
        "\n",
        "def detect_lang_croup(df:pd.Series) -> None:\n",
        "    from langdetect import detect_langs\n",
        "    temp = df.str.replace('[.,\\/#!?$%\\^&\\*;:{}=\\-_`~()\"0-9]','',regex = True)\n",
        "    temp = temp.str.replace('[\\u200f]','',regex = True)\n",
        "    temp = temp.replace(r'^\\s*$', np.nan, regex=True) # remove cell contain only spaces\n",
        "    temp = temp.replace('[]',np.nan) #remove []\n",
        "    temp = temp[temp.notnull()]\n",
        "    result = temp.apply(lambda x: detect_langs(x)[0].lang)\n",
        "    result = result.value_counts()\n",
        "    result = result.to_frame().reset_index()\n",
        "    return result\n",
        "\n",
        "\n",
        "def Zipf_law(df:pd.Series):\n",
        "    from sklearn.feature_extraction.text import CountVectorizer\n",
        "    df = df.replace('[^\\u0590-\\u05fe a-zA-Z]',' ',regex=True)\n",
        "    # Preparations\n",
        "    vec = CountVectorizer(ngram_range=(1, 1)).fit(df)\n",
        "    bag_of_words = vec.transform(df)\n",
        "    sum_words = bag_of_words.sum(axis=0)\n",
        "\n",
        "    temp = pd.DataFrame(zip(vec.get_feature_names(),bag_of_words.sum(axis=0).tolist()[0]))\n",
        "    temp.columns = ['words','freq']\n",
        "    temp = temp.sort_values('freq',ascending= False)\n",
        "    temp  = temp.reset_index(drop = True).reset_index(drop = False)\n",
        "    temp = temp.rename(columns = {'index':'rank'})\n",
        "    temp['rank'] = temp['rank'] + 1\n",
        "    temp['Zipf'] = (temp.loc[0,'freq']/temp['rank'])\n",
        "\n",
        "    temp['freq'] = temp['freq']/temp['freq'].sum()\n",
        "    temp['Zipf'] = temp['Zipf']/temp['Zipf'].sum()\n",
        "    \n",
        "    return temp\n",
        "\n",
        "\n",
        "def lexical_density(dirc:str):\n",
        "    dct_lxl = pd.read_csv(main_path + '/Colab Notebooks/Lexical_density.csv')\n",
        "    dct_lxl = dct_lxl['stopswords'].to_list()\n",
        "    a = path_main + '/' + dirc + '/ngram ' + str(1) + '.pickle'\n",
        "    with open(a, 'rb') as handle:\n",
        "        temp1 = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "    lxl = temp1[temp1['word'].isin(dct_lxl)]['frequency'].sum()/temp1['frequency'].sum()\n",
        "    return lxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "hidden": true
              }
            }
          }
        },
        "id": "zjg3B0Fqi7Or",
        "outputId": "a8023c4e-fac7-4b53-af59-feaec017c772",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bible.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "36.988131284713745\n",
            "doctors.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "118.83844590187073\n",
            "foodWalla.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "196.85246562957764\n",
            "foodpage.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "57.03791332244873\n",
            "haaretz.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "2721.2194876670837\n",
            "infomad.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "91.06147885322571\n",
            "knesset.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "1553.6615071296692\n",
            "paraShoot.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "33.79060411453247\n",
            "sentiment_Data.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n",
            "30\n",
            "35\n",
            "40\n",
            "45\n",
            "69.12683367729187\n",
            "sport5.csv\n",
            "5\n",
            "10\n",
            "15\n",
            "20\n",
            "25\n"
          ]
        }
      ],
      "source": [
        "# save the data\n",
        "\n",
        "def database_comparison(df,df_altr,fun,titl):\n",
        "    if df_altr.shape[0] > 0:\n",
        "        dat = pd.DataFrame({'shape' : [fun(df['line']), fun(df_altr['line'])],'name' : ['main df','alternative df']})\n",
        "        fig = px.pie(dat, values='shape', names='name',title = titl)\n",
        "        fig.update_traces(hoverinfo='label+percent', textinfo='value')\n",
        "        return fig\n",
        "    else:\n",
        "        return fun(df)\n",
        "\n",
        "def save_pickle(obj,file_name):\n",
        "    import pickle\n",
        "    with open(file_name, 'wb') as handle:\n",
        "                pickle.dump(obj, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "            \n",
        "def save_function(df : pd.DataFrame,dirc:str):\n",
        "    df = df[df['line'].notnull()]\n",
        "    df['line'] = df['line'].astype('str')\n",
        "    \n",
        "    #len_word\n",
        "    temp = len_word(df['line'])\n",
        "    save_pickle(temp,'len words.pickle')\n",
        "    #Zipf_law\n",
        "    temp = Zipf_law(df['line'])\n",
        "    save_pickle(temp,'Zipf law.pickle')\n",
        "    #number lines\n",
        "    temp = number_lines(df['line'])\n",
        "    save_pickle(temp,'number lines.pickle')\n",
        "    #Character distribution\n",
        "    temp = char_freq(df['line'],True)\n",
        "    save_pickle(temp,'Character distribution.pickle')\n",
        "    #ngram\n",
        "    for x in range(1,6):\n",
        "        temp = ngram(df['line'],x)\n",
        "        save_pickle(temp,'ngram ' + str(x) + '.pickle')\n",
        "    # #top 5 words and another stat\n",
        "    # temp = ngram(df['line'],1)\n",
        "    # save_pickle(temp,'another stat.pickle')\n",
        "    #identity duplicate line\n",
        "    temp = identity_duplicate_line(df['line'])\n",
        "    save_pickle(temp,'identity duplicate line.pickle')\n",
        "    #topic model\n",
        "    import time\n",
        "    start = time.time()\n",
        "    temp = topic_model(df['line'])\n",
        "    end = time.time()\n",
        "    print(end - start)  \n",
        "    save_pickle(temp,'topic model.pickle')\n",
        "    #number word in line\n",
        "    temp = number_word_in_line(df['line'])\n",
        "    save_pickle(temp,'number word in line.pickle')\n",
        "    #Language recognition in corpus\n",
        "    temp = detect_lang_croup(df['line'])\n",
        "    save_pickle(temp,'Language recognition in corpus.pickle')\n",
        "    #lexical_density\n",
        "    temp = lexical_density(dirc.split('.')[0])\n",
        "    save_pickle(temp,'lexical_density.pickle')\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "def load_database():\n",
        "    files = np.sort(os.listdir(path_corp))[12:]\n",
        "    for file in files:\n",
        "        # if file in ['Arutz 7 artical.csv','Arutz 7 news.csv','HeBERT.csv','Hebrew Dotted Text.csv','MDTEL.csv','Morphological labeling.csv','Nature of Healing.csv','To Be Healthy articles.csv',\n",
        "        # 'To Be Healthy forum.csv','ben yehuda.csv','bible.csv','doctors.csv','foodWalla.csv','foodpage.csv','haaretz.csv','infomad.csv','knesset.csv','paraShoot.csv','sentiment_Data.csv','sentiment_Data.csv'\n",
        "        # ,'sport5.csv','sportWalla.csv','tapuz.csv','themarker.csv']:\n",
        "        #     continue\n",
        "        if (file.endswith('.csv') or file.endswith('.tsv')):\n",
        "            set_cd(file.split('.')[0])\n",
        "            print(file)\n",
        "            df = pd.read_csv(path_corp + '/'+ file,sep = '\\t')\n",
        "            # df = df.sample(5)\n",
        "            # if file in ['Morphological labeling.csv','sentiment_Data.csv']:\n",
        "            #     df = pd.read_csv(path_corp + '/'+ file,sep = '\\t')\n",
        "            save_function(df,file)\n",
        "load_database()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "col": 0,
                "height": 3,
                "row": 18,
                "width": 10
              }
            }
          }
        },
        "id": "9wSdS22li7Ot"
      },
      "source": [
        "# Dataset information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSRipOdErkpP"
      },
      "outputs": [],
      "source": [
        "dirc1,ngram = 'ben yehuda',1\n",
        "dirc2 = 'think IL'\n",
        "\n",
        "def load_pickle(dirc:str,file: str):\n",
        "    if stop_words_box.value:\n",
        "        stop_words_path = 'without stopwords'\n",
        "    else:\n",
        "        stop_words_path = 'with stopwords'\n",
        "    a = main_path + '/corpus/for display' + '/'+ stop_words_path + '/' + dirc + '/' + file\n",
        "    with open(a, 'rb') as handle:\n",
        "        return pickle.load(handle)\n",
        "\n",
        "\n",
        "#data\n",
        "lst_data = ['ben yehuda','think IL','paraShoot','Hebrew-Sentiment','HeBERT Emotion Recognition','wikipedia'\n",
        "            ,'Arutz 7 (mila)','Doctors (mila)','Foodpage Corpus (mila)','haaretz (mila)','HaKnesset (mila)',\n",
        "            'Hebrew Dotted Text (mila)','Infomed (mila)','Learning Man (mila)','Nature of Healing (mila)',\n",
        "            'Spoken Israeli Hebrew (mila)','Sport5 Corpus (mila)','Tapuz People Forum (mila)','TheMarker (mila)',\n",
        "            'To Be Healthy (mila)','Walla Food Corpus (mila)']\n",
        "data_widget = widgets.Dropdown(\n",
        "    options = lst_data,\n",
        "    description='main dataset',\n",
        "    value = 'ben yehuda'\n",
        ")\n",
        "\n",
        "data_widget2 = widgets.Dropdown(\n",
        "    options = lst_data,\n",
        "    description='secondary dataset',\n",
        "    value = 'think IL'\n",
        ")\n",
        "\n",
        "#ngram\n",
        "ngram_slider = widgets.IntSlider(\n",
        "    description = 'n-gram',\n",
        "    min=1,\n",
        "    max=5,\n",
        "    step=1,\n",
        "    value=1\n",
        ")\n",
        "\n",
        "#stop words\n",
        "stop_words_box = widgets.Checkbox(True, description='Remove Stop Words')\n",
        "\n",
        "\n",
        "# co-occurrence\n",
        "word_list = load_pickle(dirc1,'ngram 1.pickle')['top unqiue words']['word'].head().to_list()\n",
        "co_widget = widgets.Dropdown(\n",
        "    options=word_list,\n",
        "    description='select word'\n",
        "    )\n",
        "\n",
        "custom_widget = widgets.Text(\n",
        "    value='דוגמא',\n",
        "    placeholder='Paste ticket description here!',\n",
        "    description='enter word:',\n",
        "    disabled=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT-TF2tOi7Oy",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "\n",
        "colors = ['indianred','lightblue']\n",
        "\n",
        "word_co = load_pickle(dirc1,'ngram 1.pickle')['top unqiue words']['word'].head().to_list()[0]\n",
        "word_co2 = load_pickle(dirc2,'ngram 1.pickle')['top unqiue words']['word'].head().to_list()[0]\n",
        "\n",
        "#Character distribution\n",
        "def chr_freq_fun(dirc:str,color):\n",
        "    chr_freq = load_pickle(dirc,'Character distribution.pickle')\n",
        "    chr_freq_row = pd.DataFrame(chr_freq.items(),columns = ['char','freq'])\n",
        "    chr_freq_row = chr_freq_row[chr_freq_row['char'].isin(list('אבגדהוזחטיכךלמםנןסעפףצץקרשת'))]\n",
        "    chr_freq_row['freq'] = chr_freq_row['freq']/chr_freq_row['freq'].sum()\n",
        "    chr_freq_plt = go.Bar(marker_color=colors[color],\n",
        "                          y=chr_freq_row['char'],\n",
        "                          x=chr_freq_row['freq'],\n",
        "                          orientation='h')\n",
        "    return chr_freq_plt\n",
        "\n",
        "chr_freq_plt = go.Figure(data=[chr_freq_fun(dirc1,0),chr_freq_fun(dirc2,1)])\n",
        "chr_freq_plt.update_layout(showlegend=False)\n",
        "chr_freq_plt = go.FigureWidget(chr_freq_plt)\n",
        "chr_freq_plt.layout.title.text = 'Character Distribution'\n",
        "chr_freq_plt.layout.title.x = 0.5\n",
        "chr_freq_plt.layout.title.y = 0.85\n",
        "\n",
        "\n",
        "chr_freq1 = load_pickle(dirc1,'Character distribution.pickle')\n",
        "chr_freq_row1 = pd.DataFrame(chr_freq1.items(),columns = ['char','freq'])\n",
        "chr_freq2 = load_pickle(dirc2,'Character distribution.pickle')\n",
        "chr_freq_row2 = pd.DataFrame(chr_freq2.items(),columns = ['char','freq'])\n",
        "chr_freq_not1 = chr_freq_row1[~chr_freq_row1['char'].isin(list('אבגדהוזחטיכךלמםנןסעפףצץקרשת'))].shape[0]\n",
        "chr_freq_not2 = chr_freq_row2[~chr_freq_row2['char'].isin(list('אבגדהוזחטיכךלמםנןסעפףצץקרשת'))].shape[0]\n",
        "unuse_char_plt = go.Bar(\n",
        "    y=[' ','  '],\n",
        "    x=[chr_freq_not1,chr_freq_not2],\n",
        "    orientation='h',\n",
        "    marker_color=colors)\n",
        "unuse_char_plt = go.Figure(unuse_char_plt).update_xaxes(type=\"log\")\n",
        "unuse_char_plt = go.FigureWidget(unuse_char_plt)\n",
        "unuse_char_plt.layout.title.text = 'Number of characters that are noise'\n",
        "unuse_char_plt.layout.title.y = 0.85\n",
        "\n",
        "\n",
        "#ngram\n",
        "def ngram_plt_fun(dirc:str,ngram:int,number_return,clr):\n",
        "    a = path_main + '/' + dirc + '/ngram ' + str(ngram) + '.pickle'\n",
        "    with open(a, 'rb') as handle:\n",
        "        temp = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "        temp['precent'] = temp['frequency']/temp['frequency'].sum()\n",
        "        temp = temp.sort_values('frequency',ascending = False).head(number_return).sort_values('frequency')\n",
        "        ngram_plot = go.Bar(name = clr,\n",
        "                y=temp['word'],\n",
        "                x=temp['precent'],\n",
        "                customdata = temp['frequency'],\n",
        "                orientation='h',\n",
        "                hovertemplate=\"<b>%{y}</b> <b>Absolute Number : %{customdata}</b>\",\n",
        "                marker_color=colors[clr]\n",
        "                )\n",
        "        return ngram_plot\n",
        "\n",
        "ngram_plot = go.Figure([ngram_plt_fun(dirc1,ngram,10,0),ngram_plt_fun(dirc2,ngram,10,1)]) \n",
        "ngram_plot.update_layout(showlegend=False)\n",
        "ngram_plot = go.FigureWidget(ngram_plot)\n",
        "ngram_plot.layout.title.text = 'ngram'\n",
        "ngram_plot.layout.title.x = 0.5\n",
        "ngram_plot.layout.title.y = 0.928\n",
        "\n",
        "#Word frequency search\n",
        "def ngram_custom_fun(dirc1:str,dric2:str,word:str):\n",
        "    a = path_main + '/' + dirc1 + '/ngram ' + str(ngram) + '.pickle'\n",
        "    with open(a, 'rb') as handle:\n",
        "        temp1 = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "        temp1['precent'] = temp1['frequency']/temp1['frequency'].sum()\n",
        "        temp1 = temp1[temp1['word'] == word]\n",
        "    \n",
        "    a = path_main + '/' + dirc2 + '/ngram ' + str(ngram) + '.pickle'\n",
        "    with open(a, 'rb') as handle:\n",
        "        temp2 = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "        temp2['precent'] = temp2['frequency']/temp2['frequency'].sum()\n",
        "        temp2 = temp2[temp2['word'] == word]\n",
        "    \n",
        "    ngram_plot = go.Bar(y=[dirc1,dirc2],\n",
        "                        x=[temp1['precent'].item(),temp2['precent'].item()],\n",
        "                        customdata = [temp1['frequency'].item(),temp2['frequency'].item()],\n",
        "                         hovertemplate=\"<b>Absolute Number : %{customdata}</b>\",\n",
        "                        orientation='h',\n",
        "                       marker_color=colors)\n",
        "    return ngram_plot\n",
        "custom_plot = go.Figure(ngram_custom_fun(dirc1,dirc2,'דוגמא'))\n",
        "custom_plot.update_layout(showlegend=False)\n",
        "custom_plot = go.FigureWidget(custom_plot)\n",
        "custom_plot.layout.title.text = 'custom ngram:'\n",
        "custom_plot.layout.title.x = 0.5\n",
        "custom_plot.layout.title.y = 0.85\n",
        "\n",
        "\n",
        "def stat_fun(dirc):\n",
        "    stat = load_pickle(dirc,'another stat.pickle')\n",
        "    stat.pop('top unqiue words',None)\n",
        "    stat['Average len words'] = load_pickle(dirc,'len words.pickle')\n",
        "    stat['number lines'] = load_pickle(dirc,'number lines.pickle')\n",
        "    stat['Percent appeared once'] = np.round(stat['Percent appeared once'],2)\n",
        "    stat['type-token ratio'] = np.round(stat['type-token ratio'],2)\n",
        "    stat['Average len words'] = np.round(stat['Average len words'],2)\n",
        "    stat['gini'] = np.round(stat['gini'],2)\n",
        "    \n",
        "    #addition stat on lines\n",
        "    addition = load_pickle(dirc,'number word in line.pickle')\n",
        "    stat['mean line'] = addition['mean']\n",
        "    stat['median line'] = addition['median']\n",
        "    return stat\n",
        "\n",
        "stat1 = stat_fun(dirc1)\n",
        "stat2 = stat_fun(dirc2)\n",
        "\n",
        "#stat\n",
        "def fun_plt_stat(stat1,stat2,txt):\n",
        "    plt_temp = go.Bar(y=[' ','  '],\n",
        "                            x=[stat1,stat2],\n",
        "                            orientation='h',\n",
        "                             marker_color=colors)\n",
        "    return plt_temp\n",
        "\n",
        "unq_plt = fun_plt_stat(stat1['shape unique words'],stat2['shape unique words'],'Number Unique Words')\n",
        "apr_plt = fun_plt_stat(stat1['shape appeared once'],stat2['shape appeared once'],'Number Appeares Once')\n",
        "prcnt_plt = fun_plt_stat(stat1['Percent appeared once'],stat2['Percent appeared once'],'Percent Appeared Once')\n",
        "nmbr_plt = fun_plt_stat(stat1['Number Of Words'],stat2['Number Of Words'],'Number Of Words')\n",
        "typ_tkn_plt = fun_plt_stat(stat1['type-token ratio'],stat2['type-token ratio'],'Type-Token Ratio')\n",
        "gini_plt = fun_plt_stat(stat1['gini'],stat2['gini'],'Gini')\n",
        "len_words_plt = fun_plt_stat(stat1['Average len words'],stat2['Average len words'],'Average len words')\n",
        "nmbr_line_plt = fun_plt_stat(stat1['number lines'],stat2['number lines'],'Number Lines')\n",
        "mean_line_plt = fun_plt_stat(stat1['mean line'],stat2['mean line'],'Mean Line')\n",
        "mdian_line_plt = fun_plt_stat(stat1['median line'],stat2['median line'],'Median Line')\n",
        "\n",
        "\n",
        "#Language recognition in corpus\n",
        "def prcnt_not_hebrew(df: pd.DataFrame):\n",
        "    df['line'] = df['line']/df['line'].sum()\n",
        "    df = df[df['index'] != 'he']\n",
        "    temp = df['line'].sum()\n",
        "    return temp\n",
        "temp = load_pickle(dirc1,'Language recognition in corpus.pickle')\n",
        "a = prcnt_not_hebrew(temp)\n",
        "temp = load_pickle(dirc2,'Language recognition in corpus.pickle')\n",
        "b = prcnt_not_hebrew(temp)\n",
        "lang_plot = fun_plt_stat(a,b,'Language recognition in corpus')\n",
        "\n",
        "\n",
        "\n",
        "#Zipf law\n",
        "Zipf_plt = go.Figure()\n",
        "#main\n",
        "temp = load_pickle(dirc1,'Zipf law.pickle').head(20)\n",
        "Zipf_plt.add_trace(go.Scatter(\n",
        "                        x = temp['rank'],\n",
        "                        y = temp['freq'],\n",
        "                        name = 'freq ' + dirc1,\n",
        "                        text = temp['words'],\n",
        "                        marker_color=colors[0]),\n",
        ")\n",
        "#secondary\n",
        "temp = load_pickle(dirc2,'Zipf law.pickle').head(20)\n",
        "Zipf_plt.add_trace(go.Scatter(\n",
        "                        x = temp['rank'],\n",
        "                        y = temp['freq'],\n",
        "                        name = 'freq ' + dirc2,\n",
        "                        text = temp['words'],\n",
        "                        marker_color=colors[1]),\n",
        ")\n",
        "\n",
        "Zipf_plt.add_trace(go.Scatter(\n",
        "                        x = temp['rank'],\n",
        "                        y = temp['Zipf'],\n",
        "                        name = 'Zipf Law',\n",
        "                        marker_color='gold'),\n",
        ")\n",
        "Zipf_plt.update_layout(showlegend=False)\n",
        "Zipf_plt = go.FigureWidget(Zipf_plt)    \n",
        "Zipf_plt.layout.title.text = 'Zipf Law'\n",
        "Zipf_plt.layout.title.x = 0.5\n",
        "Zipf_plt.layout.title.y = 0.85\n",
        "\n",
        "#topic\n",
        "temp = load_pickle(dirc1,'topic model.pickle')\n",
        "topic_plt = go.Table(header=dict(values=['Topic 1','Topic 2','Topic 3','Topic 4']),\n",
        "                 cells=dict(values=temp))\n",
        "topic_plt = go.FigureWidget(topic_plt) \n",
        "topic_plt.layout.title.text = 'Topics'\n",
        "topic_plt.layout.title.x = 0.5\n",
        "topic_plt.layout.title.y = 0.85\n",
        "\n",
        "\n",
        "\n",
        "fig = go.FigureWidget()\n",
        "\n",
        "def response_data(change):\n",
        "    dirc1 = data_widget.value\n",
        "    dirc2 = data_widget2.value\n",
        "    ngram_int = ngram_slider.value\n",
        "    word_co = co_widget.value\n",
        "    new_word = custom_widget.value\n",
        "    if stop_words_box.value:\n",
        "        stop_words_path = 'without stopwords'\n",
        "    else:\n",
        "        stop_words_path = 'with stopwords'\n",
        "    response_ngram(' ')\n",
        "    if word_co == 'Select':\n",
        "        pass\n",
        "    else:\n",
        "        with fig.batch_update():\n",
        "            #stat\n",
        "            sstat1 = stat_fun(dirc1)\n",
        "            stat2 = stat_fun(dirc2)\n",
        "\n",
        "            fig_stat.data[0].x = [stat1['shape unique words'],stat2['shape unique words']]\n",
        "            fig_stat.data[1].x = [stat1['shape appeared once'],stat2['shape appeared once']]\n",
        "            fig_stat.data[2].x = [stat1['Percent appeared once'],stat2['Percent appeared once']]\n",
        "            fig_stat.data[3].x = [stat1['Number Of Words'],stat2['Number Of Words']]\n",
        "            fig_stat.data[4].x = [stat1['type-token ratio'],stat2['type-token ratio']]\n",
        "            fig_stat.data[5].x = [stat1['gini'],stat2['gini']]\n",
        "            fig_stat.data[6].x = [stat1['Average len words'],stat2['Average len words']]\n",
        "            fig_stat.data[7].x = [stat1['number lines'],stat2['number lines']]\n",
        "            fig_stat.data[8].x = [stat1['mean line'],stat2['shape unique words']]\n",
        "            fig_stat.data[9].x = [stat1['median line'],stat2['shape unique words']]\n",
        "\n",
        "            temp = load_pickle(dirc1,'Language recognition in corpus.pickle')\n",
        "            a = prcnt_not_hebrew(temp)\n",
        "            temp = load_pickle(dirc2,'Language recognition in corpus.pickle')\n",
        "            b = prcnt_not_hebrew(temp)\n",
        "            fig_stat.data[10].x = [a,b]\n",
        "            \n",
        "            #Character distribution\n",
        "            ##main\n",
        "            def chr_update(dirc,i):\n",
        "                chr_freq = load_pickle(dirc,'Character distribution.pickle')\n",
        "                chr_freq_row = pd.DataFrame(chr_freq.items(),columns = ['char','freq'])\n",
        "                chr_freq = chr_freq_row[chr_freq_row['char'].isin(list('אבגדהוזחטיכךלמםנןסעפףצץקרשת'))]\n",
        "                chr_freq['freq'] = chr_freq['freq']/chr_freq['freq'].sum()\n",
        "                chr_freq_plt.data[i].x = chr_freq['freq']\n",
        "                chr_freq_plt.data[i].y = chr_freq['char']\n",
        "                chr_freq_plt.data[i].name = dirc\n",
        "            chr_update(dirc1,0)\n",
        "            chr_update(dirc2,1)\n",
        "\n",
        "            \n",
        "            ##unuse\n",
        "            chr_freq_not1 = chr_freq_row1[~chr_freq_row1['char'].isin(list('אבגדהוזחטיכךלמםנןסעפףצץקרשת'))].shape[0]\n",
        "            chr_freq_not2 = chr_freq_row2[~chr_freq_row2['char'].isin(list('אבגדהוזחטיכךלמםנןסעפףצץקרשת'))].shape[0]\n",
        "            unuse_char_plt.data[0].x = [chr_freq_not1,chr_freq_not2]\n",
        "            \n",
        "            #topic model\n",
        "            temp = load_pickle(dirc1,'topic model.pickle')\n",
        "            topic_plt.data[0].cells = dict(values=temp)\n",
        "            \n",
        "            #Zipf law\n",
        "            ##main\n",
        "            temp = load_pickle(dirc1,'Zipf law.pickle').head(20)\n",
        "            Zipf_plt.data[0].x = temp['rank']\n",
        "            Zipf_plt.data[0].y = temp['freq']\n",
        "            Zipf_plt.data[0].text = temp['words']\n",
        "            \n",
        "            ##secondary\n",
        "            temp = load_pickle(dirc2,'Zipf law.pickle').head(20)\n",
        "            Zipf_plt.data[1].x = temp['rank']\n",
        "            Zipf_plt.data[1].y = temp['freq']\n",
        "            Zipf_plt.data[1].text = temp['words']\n",
        "            \n",
        "            #new word custom frequency\n",
        "            def update_custom(dirc:str,new_word:str):\n",
        "                colors = ['indianred','lightblue']\n",
        "                ngram_plot.data[0].marker.color = colors[0]\n",
        "                ngram_plot.data[1].marker.color = colors[1]\n",
        "                ngram_int = len(new_word.split(' '))\n",
        "                if ngram_int >=5:\n",
        "                    ngram_int = 1\n",
        "                a = main_path + '/corpus/for display' + '/'+ stop_words_path + '/' + dirc + '/ngram ' + str(ngram_int) + '.pickle'\n",
        "                with open(a, 'rb') as handle:\n",
        "                    temp = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "                    temp['precent'] = temp['frequency']/temp['frequency'].sum()\n",
        "                    temp = temp[temp['word'] == new_word]\n",
        "                    if temp.shape[0] > 0:\n",
        "                        return temp\n",
        "                    else:\n",
        "                        return pd.DataFrame({'word':[0],'frequency':[0],'precent':[0]})\n",
        "            temp1 = update_custom(dirc1,new_word)\n",
        "            temp2 = update_custom(dirc2,new_word)\n",
        "            custom_plot.data[0].x = [temp1['precent'].item(),temp2['precent'].item()]\n",
        "            custom_plot.data[0].y = [dirc1,dirc2]\n",
        "            custom_plot.data[0].customdata = [temp1['frequency'].item(),temp2['frequency'].item()]\n",
        "            \n",
        "\n",
        "        \n",
        "\n",
        "def response_ngram(change):\n",
        "    dirc1 = data_widget.value\n",
        "    dirc2 = data_widget2.value\n",
        "    ngram_int = ngram_slider.value\n",
        "    word_co = co_widget.value\n",
        "    with fig.batch_update():\n",
        "        #ngram\n",
        "        #main\n",
        "        if stop_words_box.value:\n",
        "            stop_words_path = 'without stopwords'\n",
        "        else:\n",
        "            stop_words_path = 'with stopwords'\n",
        "        a = main_path + '/corpus/for display' + '/'+ stop_words_path + '/' + dirc1 + '/ngram ' + str(ngram_int) + '.pickle'\n",
        "        with open(a, 'rb') as handle:\n",
        "            temp1 = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "            temp1['precent'] = temp1['frequency']/temp1['frequency'].sum()\n",
        "            temp1 = temp1.sort_values('frequency',ascending = False).head(10).sort_values('frequency')   \n",
        "            ngram_plot.data[0].x = temp1['precent']\n",
        "            ngram_plot.data[0].y = temp1['word']\n",
        "            ngram_plot.data[0].customdata = temp1['frequency']\n",
        "        \n",
        "        #secondary\n",
        "        a = main_path + '/corpus/for display' + '/'+ stop_words_path + '/' + dirc2 + '/ngram ' + str(ngram_int) + '.pickle'\n",
        "        with open(a, 'rb') as handle:\n",
        "            temp2 = pickle.load(handle)['top unqiue words'].sort_values('frequency')\n",
        "            temp2['precent'] = temp2['frequency']/temp2['frequency'].sum()\n",
        "            temp2 = temp2.sort_values('frequency',ascending = False).head(10).sort_values('frequency')   \n",
        "            ngram_plot.data[1].x = temp2['precent']\n",
        "            ngram_plot.data[1].y = temp2['word']  \n",
        "            ngram_plot.data[1].customdata = temp2['frequency']\n",
        "\n",
        "        \n",
        "        \n",
        "ngram_slider.observe(response_ngram, names=\"value\")\n",
        "data_widget.observe(response_data, names=\"value\")\n",
        "data_widget2.observe(response_data, names=\"value\")\n",
        "stop_words_box.observe(response_data, names=\"value\")\n",
        "custom_widget.on_submit(response_data)\n",
        "\n",
        "#ngram click\n",
        "\n",
        "def update_point(trace, points, selector):\n",
        "    colors = ['indianred','lightblue']\n",
        "    if len(points.ys) > 0:\n",
        "        word = points.ys[0]\n",
        "        graph = int(trace.name)\n",
        "        lst_word = trace.y\n",
        "        \n",
        "        #update custom plot\n",
        "        custom_widget.value = word\n",
        "        response_data(' ')\n",
        "        \n",
        "        #change bar color\n",
        "        index_change = np.where(word == lst_word)[0][0]         \n",
        "        if graph == 0:\n",
        "            ngram_plot.data[1].marker.color = [colors[1]]*10\n",
        "            color = [colors[0]]*10\n",
        "        if graph == 1:\n",
        "            ngram_plot.data[0].marker.color = [colors[0]]*10\n",
        "            color = [colors[1]]*10\n",
        "        color[index_change] = 'lightslategray'\n",
        "        ngram_plot.data[graph].marker.color = color\n",
        "\n",
        "nargm_click1 = ngram_plot.data[0]\n",
        "nargm_click2 = ngram_plot.data[1]\n",
        "nargm_click1.on_click(update_point)\n",
        "nargm_click2.on_click(update_point)\n",
        "\n",
        "\n",
        "\n",
        "title = ['Number Unique Words','Number Appeares Once','Percent Appeared Once','Number Of Words','Type-Token Ratio','Gini','Average len words'\n",
        ",'Number Lines','Mean Line','Median Line','Language recognition in corpus']\n",
        "fig_stat = make_subplots(specs=[[{\"type\": \"bar\"}]*4]*3,\n",
        "            rows=3, cols=4,\n",
        "            subplot_titles = title)\n",
        "index = 0\n",
        "lst = [unq_plt,apr_plt,prcnt_plt,nmbr_plt,typ_tkn_plt,gini_plt,len_words_plt,nmbr_line_plt,mean_line_plt,mdian_line_plt,lang_plot]\n",
        "for i in range(1,4):\n",
        "    for j in range(1,5):\n",
        "        fig_stat.add_trace(lst[index],row=i, col=j)\n",
        "        index+=1\n",
        "        if index == len(lst):\n",
        "            break\n",
        "fig_stat.update_layout(height=600,width = 1000, showlegend=False)\n",
        "fig_stat = go.FigureWidget(fig_stat)\n",
        "\n",
        "ngram_plot.layout.height = 800\n",
        "custom_plot.layout.height = 400\n",
        "\n",
        "#ngram + custom box\n",
        "custom_plot_container = widgets.VBox([custom_widget,\n",
        "                             custom_plot\n",
        "                                     ])\n",
        "\n",
        "ngram_container = widgets.VBox([ngram_slider,\n",
        "                                ngram_plot,\n",
        "                                ])\n",
        "ngram_custom_box = widgets.HBox([ngram_container,custom_plot_container])\n",
        "top_widget = widgets.HBox([data_widget,data_widget2,stop_words_box])\n",
        "\n",
        "box = widgets.HBox([chr_freq_plt,unuse_char_plt,Zipf_plt])\n",
        "grid = widgets.VBox([top_widget,fig_stat,ngram_custom_box,topic_plt,box])\n",
        "grid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qg_blGj-zIoy"
      },
      "outputs": [],
      "source": [
        "# distribtion for words\n",
        "import itertools\n",
        "for index in itertools.combinations(lst_data, 2):\n",
        "    dis_word1 = load_pickle(index[0],'ngram 1.pickle')['top unqiue words']\n",
        "    dis_word1 = dis_word1['frequency']/dis_word1['frequency'].sum()\n",
        "    dis_word2 = load_pickle(index[1],'ngram 1.pickle')['top unqiue words']\n",
        "    dis_word2 = dis_word2['frequency']/dis_word2['frequency'].sum()\n",
        "    dis_word2\n",
        "\n",
        "\n",
        "    def lng_distubtion_function(a,b):\n",
        "        from scipy.special import rel_entr\n",
        "        if a.shape[0] > b.shape[0]:\n",
        "            a = a.sample(b.shape[0]).reset_index(drop = True)\n",
        "        else:\n",
        "            b = b.sample(a.shape[0]).reset_index(drop = True)\n",
        "        return sum(rel_entr(a, b)) + sum(rel_entr(b, a))\n",
        "\n",
        "    lng_distubtion_function(dis_word1,dis_word2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6T6k_4EzIo1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(index = lst_data,columns = lst_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "col": 0,
                "height": 2,
                "row": 25,
                "width": 12
              }
            }
          }
        },
        "id": "hGkX6dzzi7O2"
      },
      "source": [
        "# Focused look at a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "col": 2,
                "height": 10,
                "row": 41,
                "width": 7
              }
            }
          }
        },
        "id": "JjIs25ZEi7O2",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def over_view(df: pd.DataFrame,col : str, n : int,plot_text : str,plot = 'none'):\n",
        "    temp = df[col].value_counts().reset_index().sort_values(col,ascending=False).reset_index(drop = True)\n",
        "    temp = temp.replace(r'\\s+', ' ', regex=True)\n",
        "    temp.loc[temp['index'] == ' ','index'] = 'לא ידוע'\n",
        "    sum_all = temp[col].sum()\n",
        "    temp = temp[(temp[col]/temp[col].sum()).cumsum() < 0.90]\n",
        "    temp = temp.append({'index': 'others',col: sum_all - temp[col].sum()}, ignore_index=True)\n",
        "    return temp\n",
        "        \n",
        "def remove_stop_word(df:pd.Series) -> pd.Series:\n",
        "    stop = pd.read_csv(r'C:\\corpus\\heb_stopwords.txt', sep=\" \", header=None)\n",
        "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "    return df\n",
        "\n",
        "# def sample(df,df_altr):\n",
        "#     n=5\n",
        "#     df = df.sample(n)\n",
        "#     if df_altr.shape[0] > 100:\n",
        "#         df_altr = df_altr.sample(n)\n",
        "#     return df, df_altr\n",
        "    \n",
        "\n",
        "out = widgets.Output()\n",
        "display(out)\n",
        "def focused_eventhandler(change):\n",
        "    os.chdir(r'C:\\corpus\\for display')\n",
        "    global df,main,df_altr,flag\n",
        "    if (change == 'ben yehuda'):\n",
        "        main = pd.read_csv(r'over_view/ben yehuda.csv')\n",
        "            \n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
        "                    [{\"type\": \"pie\"}, {\"type\": \"pie\"}]],\n",
        "            subplot_titles = ('authors','translators','Distribution of genres','Distribution of translated languages')\n",
        "            )\n",
        "\n",
        "        temp = over_view(main,'authors',10,' ','bar')\n",
        "        fig.add_trace(go.Bar(y=temp['index'],\n",
        "                                 x=temp['authors'],\n",
        "                                orientation='h'),\n",
        "                          row=1, col=1)\n",
        "\n",
        "        temp = over_view(main[main['translators'].notnull()],'translators',10,'Translators','bar')\n",
        "        fig.add_trace(go.Bar(y=temp['index'],\n",
        "                                 x=temp['translators'],\n",
        "                                orientation='h'),\n",
        "                          row=1, col=2)\n",
        "\n",
        "        temp = over_view(main,'genre',10,'Distribution of genres','pie')\n",
        "        fig.add_trace(go.Pie(values=temp['genre'],\n",
        "                            labels=temp['index']),\n",
        "                        row=2, col=1)\n",
        "\n",
        "        temp = over_view(main,'original_language',10,'Distribution of translated languages','pie')\n",
        "        fig.add_trace(go.Pie(values=temp['original_language'],\n",
        "                            labels=temp['index']),\n",
        "                        row=2, col=2)\n",
        "\n",
        "        fig.update_layout(height=700, showlegend=False)\n",
        "\n",
        "        fig.show()\n",
        "    elif (change == 'think il'):\n",
        "        main = pd.read_csv(r'over_view/think il.csv')\n",
        "        temp = over_view(main,'author',10,'Distribution of authors','bar')\n",
        "        fig = go.Figure(\n",
        "            data=[go.Bar(x=temp['author'], y=temp['index'], orientation='h')],\n",
        "            layout=go.Layout(\n",
        "                title=go.layout.Title(text=\"Distribution of authors\")\n",
        "            )\n",
        "        )\n",
        "        fig.show()\n",
        "        \n",
        "    elif (change == 'ParaShoot'):\n",
        "        df = pd.read_csv('C:\\corpus\\ParaShoot\\df.csv',sep = '\\t')\n",
        "        temp = df[['title']].value_counts().to_frame().reset_index()\n",
        "        temp.columns = ['title','freq']\n",
        "        fig = go.Figure(\n",
        "            data=[go.Bar(y=temp['title'], x=temp['freq'], orientation='h')],\n",
        "            layout=go.Layout(\n",
        "                title=go.layout.Title(text=\"Distribution of article\")\n",
        "            )\n",
        "        )\n",
        "        fig.show()\n",
        "        \n",
        "    elif (change == 'Hebrew-Sentiment-Data'):\n",
        "        df = pd.read_csv('C:\\corpus\\Hebrew-Sentiment-Data\\dev.tsv',sep = '\\t')\n",
        "        df = df.rename(columns = {'comment':'line'})\n",
        "        df = df['label'].value_counts().to_frame().reset_index()\n",
        "        df.columns = ['lable','count']\n",
        "        fig = go.Figure(go.Pie(values=df['count'],\n",
        "                                    labels=['Positive','negative','neutral']))\n",
        "        fig.show()\n",
        "        \n",
        "    \n",
        "    elif (change == 'HeBERT Emotion Recognition'):\n",
        "        df = pd.read_csv(r'C:\\corpus\\HeBERT Emotion Recognition\\test_insample_tagged_raw_to_publish.csv')\n",
        "        df = df.rename(columns = {'talkbacks':'line'})\n",
        "\n",
        "        fig = make_subplots(\n",
        "                            rows=1, cols=3,\n",
        "                            specs=[[{\"type\": \"pie\"}, {\"type\": \"pie\"}, {\"type\": \"pie\"}]],\n",
        "                    subplot_titles = ('Emotion Distribution','Sentiment Distribution','Source of response')\n",
        "                    )\n",
        "        temp = df['emotion_en'].value_counts().to_frame().reset_index()\n",
        "        fig.add_trace(go.Pie(values=temp['emotion_en'],\n",
        "                                    labels=temp['index']),\n",
        "                                row=1, col=1)\n",
        "\n",
        "        temp = df['value'].value_counts().to_frame().reset_index()\n",
        "        fig.add_trace(go.Pie(values=temp['value'],\n",
        "                                    labels=temp['index']),\n",
        "                                row=1, col=2)\n",
        "\n",
        "        temp = df['source'].value_counts().to_frame().reset_index()\n",
        "        fig.add_trace(go.Pie(values=temp['source'],\n",
        "                                    labels=temp['index']),\n",
        "                                row=1, col=3)\n",
        "\n",
        "        fig.update_layout(height=700, showlegend=False)\n",
        "        fig.show()\n",
        "\n",
        "\n",
        "def main_load_dataset(dirc: list):\n",
        "    create_folder('over_view')\n",
        "    #ben yehuda\n",
        "    main = pd.read_csv(r'C:\\corpus\\ben yehuda\\pseudocatalogue.csv')\n",
        "    main[['authors','genre','original_language','translators']].to_csv(r'over_view/ben yehuda.csv')\n",
        "    \n",
        "    #think il\n",
        "    main = pd.read_csv(r'C:\\corpus\\think il\\df think IL token.tsv',sep = '\\t')\n",
        "    main = main.rename(columns={'text':'line','auther':'author'})\n",
        "    main['author'].to_csv(r'over_view/think il.csv')\n",
        "\n",
        "from plotly.subplots import make_subplots\n",
        "focused_widget = widgets.Dropdown(options = ['ben yehuda','think il','ParaShoot','Hebrew-Sentiment-Data','HeBERT Emotion Recognition'],\n",
        "                                  value = None,\n",
        "                                  description='Chose data')\n",
        "focused_view = widgets.interactive(focused_eventhandler, change=focused_widget)\n",
        "focused_widget.observe(lambda x: focused_view.update(), 'value')\n",
        "display(focused_view)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "extensions": {
          "jupyter_dashboards": {
            "version": 1,
            "views": {
              "default_view": {
                "hidden": true
              }
            }
          }
        },
        "id": "dnMO60sLi7O3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "function.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "extensions": {
      "jupyter_dashboards": {
        "activeView": "default_view",
        "version": 1,
        "views": {
          "default_view": {
            "cellMargin": 10,
            "defaultCellHeight": 40,
            "maxColumns": 12,
            "name": "active_view",
            "type": "grid"
          }
        }
      }
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}